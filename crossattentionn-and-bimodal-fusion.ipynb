{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:03:47.486824Z",
     "iopub.status.busy": "2025-10-16T13:03:47.486619Z",
     "iopub.status.idle": "2025-10-16T13:05:06.242436Z",
     "shell.execute_reply": "2025-10-16T13:05:06.241505Z",
     "shell.execute_reply.started": "2025-10-16T13:03:47.486807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install transformers==4.42.4 accelerate datasets==2.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:05:06.244234Z",
     "iopub.status.busy": "2025-10-16T13:05:06.243994Z",
     "iopub.status.idle": "2025-10-16T13:05:07.134352Z",
     "shell.execute_reply": "2025-10-16T13:05:07.133584Z",
     "shell.execute_reply.started": "2025-10-16T13:05:06.244213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jefferyYu/UMT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:05:07.141896Z",
     "iopub.status.busy": "2025-10-16T13:05:07.141656Z",
     "iopub.status.idle": "2025-10-16T13:05:15.764765Z",
     "shell.execute_reply": "2025-10-16T13:05:15.763934Z",
     "shell.execute_reply.started": "2025-10-16T13:05:07.141880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import gdown\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:05:15.766032Z",
     "iopub.status.busy": "2025-10-16T13:05:15.765603Z",
     "iopub.status.idle": "2025-10-16T13:05:15.773379Z",
     "shell.execute_reply": "2025-10-16T13:05:15.772599Z",
     "shell.execute_reply.started": "2025-10-16T13:05:15.766006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_twitter_conll(path: str):\n",
    "    \"\"\"Parse Twitter2015 style: blocks separated by blank lines, starting with IMGID:xxxx\"\"\"\n",
    "    samples = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f]\n",
    "    img_id, toks, tags = None, [], []\n",
    "    for ln in lines + ['']:  # sentinel blank to flush last sample\n",
    "        if not ln.strip():\n",
    "            if img_id is not None and toks:\n",
    "                samples.append({'img_id': img_id, 'tokens': toks, 'labels': tags})\n",
    "            img_id, toks, tags = None, [], []\n",
    "            continue\n",
    "        if ln.startswith('IMGID:'):\n",
    "            img_id = ln.split(':', 1)[1].strip()\n",
    "        else:\n",
    "            # token and BIO tag separated by whitespace\n",
    "            parts = ln.split()\n",
    "            if len(parts) >= 2:\n",
    "                toks.append(parts[0])\n",
    "                tags.append(parts[1])\n",
    "    return samples\n",
    "\n",
    "def build_label_vocab(*lists_of_samples):\n",
    "    labels = set()\n",
    "    for s_list in lists_of_samples:\n",
    "        for s in s_list:\n",
    "            labels.update(s['labels'])\n",
    "    labels = sorted(labels)  # stable order\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "    id2label = {i:l for l,i in label2id.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "def find_image_path(img_dir: str, img_id: str):\n",
    "    for ext in ('.jpg', '.jpeg', '.png', '.bmp'):\n",
    "        p = os.path.join(img_dir, img_id + ext)\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:05:15.774524Z",
     "iopub.status.busy": "2025-10-16T13:05:15.774300Z",
     "iopub.status.idle": "2025-10-16T13:05:15.874226Z",
     "shell.execute_reply": "2025-10-16T13:05:15.873650Z",
     "shell.execute_reply.started": "2025-10-16T13:05:15.774508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== Setup & Imports ==================\n",
    "# !pip -q install transformers==4.42.4 accelerate datasets==2.21.0\n",
    "\n",
    "import os, random, math, json, numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaModel, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ------------------- Repro -------------------\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================== Paths (your config) ==================\n",
    "TXT_PATHS = {\n",
    "    'train': '/kaggle/working/UMT/data/twitter2015/train.txt',\n",
    "    'val':   '/kaggle/working/UMT/data/twitter2015/valid.txt',\n",
    "    'test':  '/kaggle/working/UMT/data/twitter2015/test.txt'\n",
    "}\n",
    "IMG_DIRS = {\n",
    "    'train': '/kaggle/input/twitter2015/twitter2015/twitter2015_images',\n",
    "    'val':   '/kaggle/input/twitter2015/twitter2015/twitter2015_images',\n",
    "    'test':  '/kaggle/input/twitter2015/twitter2015/twitter2015_images'\n",
    "}\n",
    "\n",
    "# ================== Utils ==================\n",
    "def parse_twitter_conll(path: str):\n",
    "    \"\"\"Parse Twitter2015 style: blocks separated by blank lines, starting with IMGID:xxxx\"\"\"\n",
    "    samples = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f]\n",
    "    img_id, toks, tags = None, [], []\n",
    "    for ln in lines + ['']:  # sentinel blank to flush last sample\n",
    "        if not ln.strip():\n",
    "            if img_id is not None and toks:\n",
    "                samples.append({'img_id': img_id, 'tokens': toks, 'labels': tags})\n",
    "            img_id, toks, tags = None, [], []\n",
    "            continue\n",
    "        if ln.startswith('IMGID:'):\n",
    "            img_id = ln.split(':', 1)[1].strip()\n",
    "        else:\n",
    "            # token and BIO tag separated by whitespace\n",
    "            parts = ln.split()\n",
    "            if len(parts) >= 2:\n",
    "                toks.append(parts[0])\n",
    "                tags.append(parts[1])\n",
    "    return samples\n",
    "\n",
    "def build_label_vocab(*lists_of_samples):\n",
    "    labels = set()\n",
    "    for s_list in lists_of_samples:\n",
    "        for s in s_list:\n",
    "            labels.update(s['labels'])\n",
    "    labels = sorted(labels)  # stable order\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "    id2label = {i:l for l,i in label2id.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "def find_image_path(img_dir: str, img_id: str):\n",
    "    for ext in ('.jpg', '.jpeg', '.png', '.bmp'):\n",
    "        p = os.path.join(img_dir, img_id + ext)\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# ================== Dataset ==================\n",
    "class Twitter2015MNER(Dataset):\n",
    "    def __init__(self, samples, img_dir, tokenizer: RobertaTokenizerFast, label2id, max_len=128, aug=False):\n",
    "        self.samples = samples\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if aug:\n",
    "            self.img_tfm = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomResizedCrop((224,224), scale=(0.9,1.0)),\n",
    "                transforms.ColorJitter(0.15,0.15,0.15,0.05),\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            self.img_tfm = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.samples[idx]\n",
    "        tokens: List[str] = ex['tokens']\n",
    "        labels: List[str] = ex['labels']\n",
    "\n",
    "        # Tokenize with word alignment\n",
    "        # --- Tokenize text first ---\n",
    "        encodings = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 👉 Call .word_ids BEFORE squeezing\n",
    "        word_ids = encodings.word_ids(batch_index=0)\n",
    "        \n",
    "        # Then convert tensors for PyTorch\n",
    "        enc = {k: v.squeeze(0) for k, v in encodings.items()}\n",
    "        \n",
    "        # --- Align labels with subwords ---\n",
    "        label_ids = []\n",
    "        prev_word = None\n",
    "        for w_id in word_ids:\n",
    "            if w_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif w_id != prev_word:\n",
    "                label_ids.append(self.label2id[labels[w_id]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word = w_id\n",
    "        label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "\n",
    "        # Image\n",
    "        img_path = find_image_path(self.img_dir, ex['img_id'])\n",
    "        if img_path is None:\n",
    "            # fallback: blank image if missing\n",
    "            img = Image.new('RGB', (224,224), color=(0,0,0))\n",
    "        else:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_tfm(img)\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'],\n",
    "            'attention_mask': enc['attention_mask'],\n",
    "            'pixel_values': img,\n",
    "            'labels': label_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:05:15.875184Z",
     "iopub.status.busy": "2025-10-16T13:05:15.874954Z",
     "iopub.status.idle": "2025-10-16T13:05:15.884106Z",
     "shell.execute_reply": "2025-10-16T13:05:15.883448Z",
     "shell.execute_reply.started": "2025-10-16T13:05:15.875167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RobertaResNet50MNER(nn.Module):\n",
    "    \"\"\"\n",
    "    Token classification with image-conditioned modulation (FiLM-like):\n",
    "      - Text encoder: roberta-large (hidden=1024)\n",
    "      - Image encoder: resnet50 -> 2048-d pooled -> Linear -> 1024\n",
    "      - gamma = Wg(img), beta = Wb(img)\n",
    "      - h' = (1 + gamma) * h + beta  (applied to every token)\n",
    "      - Token classifier to BIO tag space\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels, text_model='roberta-large'):\n",
    "        super().__init__()\n",
    "\n",
    "        # Text Model (RoBERTa)\n",
    "        self.text = RobertaModel.from_pretrained(text_model)\n",
    "        hidden = self.text.config.hidden_size  # 1024\n",
    "\n",
    "        # Visual Model (ResNet50)\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.visual = resnet\n",
    "\n",
    "        # Image Projection to match RoBERTa's hidden size\n",
    "        self.img_proj = nn.Linear(2048, hidden)\n",
    "\n",
    "        # Cross-attention Fusion Layer (Gamma and Beta for FiLM modulation)\n",
    "        self.gamma = nn.Linear(hidden, hidden)\n",
    "        self.beta  = nn.Linear(hidden, hidden)\n",
    "\n",
    "        # Bimodal Projection Layer (to project concatenated features to the desired dimension)\n",
    "        self.bimodal_projection = nn.Linear(2048, hidden)\n",
    "\n",
    "        # Additional Feedforward Layers (MLP) for improved learning\n",
    "        self.ff_text = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden * 2, hidden)\n",
    "        )\n",
    "\n",
    "        self.ff_image = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden * 2, hidden)\n",
    "        )\n",
    "\n",
    "        # Attention Layer to refine fusion (optional)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=8, batch_first=True)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Classifier to predict the labels\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n",
    "        # Text\n",
    "        out = self.text(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq = out.last_hidden_state  # [B, L, 1024]\n",
    "\n",
    "        # Image\n",
    "        img_feat = self.visual(pixel_values)       # [B, 2048]\n",
    "        img_feat = self.img_proj(img_feat)         # [B, 1024]\n",
    "\n",
    "        # FiLM modulation per token\n",
    "        g = self.gamma(img_feat).unsqueeze(1)      # [B, 1, 1024]\n",
    "        b = self.beta(img_feat).unsqueeze(1)       # [B, 1, 1024]\n",
    "        seq = (1 + g) * seq + b                    # [B, L, 1024]\n",
    "\n",
    "        # Bimodal Fusion: Concatenate Text and Image features\n",
    "        # Concatenate along the feature dimension (dim=-1)\n",
    "        bimodal_feats = torch.cat((seq, img_feat.unsqueeze(1).repeat(1, seq.size(1), 1)), dim=-1)  # [B, L, 2048]\n",
    "\n",
    "        # Apply Bimodal Projection to reduce the concatenated dimension to 1024\n",
    "        bimodal_feats = self.bimodal_projection(bimodal_feats)  # [B, L, 1024]\n",
    "\n",
    "        # Optionally apply attention on the fused features\n",
    "        attn_output, _ = self.attn(bimodal_feats, bimodal_feats, bimodal_feats)\n",
    "\n",
    "        # Dropout regularization\n",
    "        attn_output = self.dropout(attn_output)\n",
    "\n",
    "        # Classifier to predict the labels (BIO tags)\n",
    "        logits = self.classifier(attn_output)  # [B, L, C]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:05:15.885071Z",
     "iopub.status.busy": "2025-10-16T13:05:15.884828Z",
     "iopub.status.idle": "2025-10-16T13:27:44.995720Z",
     "shell.execute_reply": "2025-10-16T13:27:44.994852Z",
     "shell.execute_reply.started": "2025-10-16T13:05:15.885049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== Build Data ==================\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large', add_prefix_space=True)\n",
    "\n",
    "\n",
    "train_samples = parse_twitter_conll(TXT_PATHS['train'])\n",
    "val_samples   = parse_twitter_conll(TXT_PATHS['val'])\n",
    "test_samples  = parse_twitter_conll(TXT_PATHS['test'])\n",
    "\n",
    "label2id, id2label = build_label_vocab(train_samples, val_samples, test_samples)\n",
    "print(\"Labels:\", label2id)\n",
    "\n",
    "train_ds = Twitter2015MNER(train_samples, IMG_DIRS['train'], tokenizer, label2id, max_len=128, aug=True)\n",
    "val_ds   = Twitter2015MNER(val_samples,   IMG_DIRS['val'],   tokenizer, label2id, max_len=128, aug=False)\n",
    "test_ds  = Twitter2015MNER(test_samples,  IMG_DIRS['test'],  tokenizer, label2id, max_len=128, aug=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=12, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=12, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ================== Train ==================\n",
    "model = RobertaResNet50MNER(num_labels=len(label2id)).to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup = int(0.1 * total_steps)\n",
    "sched = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup, num_training_steps=total_steps)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else: model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    losses = []\n",
    "\n",
    "    it = tqdm(loader, leave=False, desc=\"Train\" if train else \"Eval\")\n",
    "    for batch in it:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn      = batch['attention_mask'].to(device)\n",
    "        pixels    = batch['pixel_values'].to(device)\n",
    "        labels    = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            logits, loss = model(input_ids, attn, pixels, labels if train else labels)\n",
    "\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optim.step()\n",
    "            sched.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # collect predictions & labels for metrics (ignore -100)\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        golds = labels.detach().cpu().numpy()\n",
    "\n",
    "        for p, g in zip(preds, golds):\n",
    "            for pi, gi in zip(p, g):\n",
    "                if gi == -100:  # skip subwords / pads\n",
    "                    continue\n",
    "                all_preds.append(pi)\n",
    "                all_labels.append(gi)\n",
    "\n",
    "    avg_loss = float(np.mean(losses))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    return avg_loss, acc, prec, rec, f1\n",
    "\n",
    "best_val_f1, best_state = -1.0, None\n",
    "patience, bad = 2, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc, tr_p, tr_r, tr_f1 = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_acc, vl_p, vl_r, vl_f1 = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train loss {tr_loss:.4f} acc {tr_acc:.3f} P {tr_p:.3f} R {tr_r:.3f} F1 {tr_f1:.3f} || \"\n",
    "          f\"Val loss {vl_loss:.4f} acc {vl_acc:.3f} P {vl_p:.3f} R {vl_r:.3f} F1 {vl_f1:.3f}\")\n",
    "\n",
    "    if vl_f1 > best_val_f1:\n",
    "        best_val_f1 = vl_f1; bad = 0\n",
    "        best_state = {\n",
    "            'model': model.state_dict(),\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label,\n",
    "            'config': {'text':'roberta-large','img':'resnet50','fusion':'FiLM'}\n",
    "        }\n",
    "        torch.save(best_state, 'roberta_resnet50_mner_best.pth')\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# ================== Evaluate on Test ==================\n",
    "# load best\n",
    "if best_state is None:\n",
    "    best_state = torch.load('roberta_resnet50_mner_best.pth', map_location=device)\n",
    "model.load_state_dict(best_state['model'])\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, leave=False, desc=\"Test\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn      = batch['attention_mask'].to(device)\n",
    "            pixels    = batch['pixel_values'].to(device)\n",
    "            labels    = batch['labels'].to(device)\n",
    "            logits, loss = model(input_ids, attn, pixels, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            golds = labels.detach().cpu().numpy()\n",
    "            for p, g in zip(preds, golds):\n",
    "                for pi, gi in zip(p, g):\n",
    "                    if gi == -100: continue\n",
    "                    all_preds.append(pi); all_labels.append(gi)\n",
    "\n",
    "    avg_loss = float(np.mean(losses))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    return avg_loss, acc, prec, rec, f1\n",
    "\n",
    "te_loss, te_acc, te_p, te_r, te_f1 = evaluate(test_loader)\n",
    "print(\"\\n===== TEST RESULTS (Token-level, ignore subwords) =====\")\n",
    "print(f\"Loss: {te_loss:.4f}\")\n",
    "print(f\"Accuracy:  {te_acc:.4f}\")\n",
    "print(f\"Precision: {te_p:.4f}\")\n",
    "print(f\"Recall:    {te_r:.4f}\")\n",
    "print(f\"F1:        {te_f1:.4f}\")\n",
    "\n",
    "# (Optional) per-label report\n",
    "def per_label_report(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn      = batch['attention_mask'].to(device)\n",
    "            pixels    = batch['pixel_values'].to(device)\n",
    "            labels    = batch['labels'].to(device)\n",
    "            logits, _ = model(input_ids, attn, pixels, labels)\n",
    "            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            golds = labels.detach().cpu().numpy()\n",
    "            for p, g in zip(preds, golds):\n",
    "                for pi, gi in zip(p, g):\n",
    "                    if gi == -100: continue\n",
    "                    all_preds.append(pi); all_labels.append(gi)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    target_names = [l for i,l in sorted(id2label.items())]\n",
    "    print(\"\\nPer-label report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, zero_division=0))\n",
    "\n",
    "# Uncomment to print per-label metrics:\n",
    "# per_label_report(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T13:27:44.997175Z",
     "iopub.status.busy": "2025-10-16T13:27:44.996866Z",
     "iopub.status.idle": "2025-10-16T13:28:31.404099Z",
     "shell.execute_reply": "2025-10-16T13:28:31.403287Z",
     "shell.execute_reply.started": "2025-10-16T13:27:44.997143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "per_label_report(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6075740,
     "sourceId": 9892595,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
