{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T07:11:42.498768Z",
     "iopub.status.busy": "2025-10-05T07:11:42.497867Z",
     "iopub.status.idle": "2025-10-05T07:11:42.513299Z",
     "shell.execute_reply": "2025-10-05T07:11:42.512572Z",
     "shell.execute_reply.started": "2025-10-05T07:11:42.498738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# your dataset image folder\n",
    "img_dir = \"/kaggle/input/twitter2015/twitter2015/twitter2015_images\"\n",
    "\n",
    "# image id (from your text file)\n",
    "img_id = \"1015799\"\n",
    "\n",
    "# possible extensions (Twitter2015 images are usually .jpg)\n",
    "possible_exts = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "\n",
    "found = False\n",
    "for ext in possible_exts:\n",
    "    img_path = os.path.join(img_dir, img_id + ext)\n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"✅ Image found: {img_path}\")\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if not found:\n",
    "    # show a few similar files in case of different naming pattern\n",
    "    print(f\"❌ Image {img_id} not found in {img_dir}\")\n",
    "    files = [f for f in os.listdir(img_dir) if img_id in f]\n",
    "    if files:\n",
    "        print(\"Possible matches:\")\n",
    "        for f in files:\n",
    "            print(\"  -\", f)\n",
    "    else:\n",
    "        print(\"No similar filenames found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T07:11:52.417101Z",
     "iopub.status.busy": "2025-10-05T07:11:52.416856Z",
     "iopub.status.idle": "2025-10-05T07:11:53.399879Z",
     "shell.execute_reply": "2025-10-05T07:11:53.398965Z",
     "shell.execute_reply.started": "2025-10-05T07:11:52.417083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jefferyYu/UMT.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter2015 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T06:05:33.877427Z",
     "iopub.status.busy": "2025-10-05T06:05:33.876708Z",
     "iopub.status.idle": "2025-10-05T06:05:33.934368Z",
     "shell.execute_reply": "2025-10-05T06:05:33.933818Z",
     "shell.execute_reply.started": "2025-10-05T06:05:33.877396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, ast, math, json, random\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
    "\n",
    "# -------------- Repro --------------\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------- Paths (EDIT if needed) --------------\n",
    "TXT_PATHS = {\n",
    "    'train': '/kaggle/working/UMT/data/twitter2015/train.txt',\n",
    "    'val':   '/kaggle/working/UMT/data/twitter2015/valid.txt',   # will auto-fix if 'mnre_val .txt' exists\n",
    "    'test':  '/kaggle/working/UMT/data/twitter2015/test.txt'\n",
    "}\n",
    "IMG_DIRS = {\n",
    "    'train': '/kaggle/input/twitter2015/twitter2015/twitter2015_images',\n",
    "    'val':   '/kaggle/input/twitter2015/twitter2015/twitter2015_images',\n",
    "    'test':  '/kaggle/input/twitter2015/twitter2015/twitter2015_images'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T06:05:38.929974Z",
     "iopub.status.busy": "2025-10-05T06:05:38.929347Z",
     "iopub.status.idle": "2025-10-05T06:05:39.487854Z",
     "shell.execute_reply": "2025-10-05T06:05:39.487020Z",
     "shell.execute_reply.started": "2025-10-05T06:05:38.929949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, re, random, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Paths (you already have these)\n",
    "TXT_PATH = \"/kaggle/working/UMT/data/twitter2015/train.txt\"\n",
    "IMG_DIR = \"/kaggle/input/twitter2015/twitter2015/twitter2015_images\"\n",
    "\n",
    "# Function to parse the Twitter2015 CoNLL file\n",
    "def parse_twitter_file(txt_path):\n",
    "    samples = []\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_imgid, tokens, tags = None, [], []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:  # blank line → end of sample\n",
    "            if current_imgid and tokens:\n",
    "                samples.append({'img_id': current_imgid, 'tokens': tokens, 'tags': tags})\n",
    "                tokens, tags = [], []\n",
    "            continue\n",
    "        if line.startswith(\"IMGID:\"):\n",
    "            current_imgid = line.split(\":\")[1].strip()\n",
    "        else:\n",
    "            try:\n",
    "                word, label = line.split()\n",
    "                tokens.append(word)\n",
    "                tags.append(label)\n",
    "            except:\n",
    "                continue\n",
    "    return samples\n",
    "\n",
    "# Load a few samples\n",
    "samples = parse_twitter_file(TXT_PATH)\n",
    "print(f\"Total samples parsed: {len(samples)}\")\n",
    "\n",
    "# Define colors for entity types\n",
    "color_map = {\n",
    "    'PER': '#FFD700',   # yellow\n",
    "    'LOC': '#90EE90',   # green\n",
    "    'ORG': '#87CEFA',   # blue\n",
    "    'OTHER': '#FFB6C1', # pink\n",
    "    'O': 'white'\n",
    "}\n",
    "\n",
    "# Helper to colorize tokens\n",
    "def colorize_tokens(tokens, tags):\n",
    "    html = \"\"\n",
    "    for tok, tag in zip(tokens, tags):\n",
    "        if tag == 'O':\n",
    "            html += f\"<span style='background-color:white'>{tok} </span>\"\n",
    "        else:\n",
    "            ent_type = tag.split('-')[-1]\n",
    "            color = color_map.get(ent_type, 'white')\n",
    "            html += f\"<span style='background-color:{color}; padding:2px; border-radius:4px;'>{tok} </span>\"\n",
    "    return html\n",
    "\n",
    "# Random visualization\n",
    "for i in range(3):\n",
    "    sample = random.choice(samples)\n",
    "    img_id = sample['img_id']\n",
    "    # check file with any extension\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        img_path = os.path.join(IMG_DIR, img_id + ext)\n",
    "        if os.path.exists(img_path):\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Image not found for ID {img_id}\")\n",
    "        continue\n",
    "    \n",
    "    # Display image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"IMGID: {img_id}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Display text with entities\n",
    "    html = colorize_tokens(sample['tokens'], sample['tags'])\n",
    "    display(HTML(html))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T06:11:01.336867Z",
     "iopub.status.busy": "2025-10-05T06:11:01.336171Z",
     "iopub.status.idle": "2025-10-05T06:32:26.536930Z",
     "shell.execute_reply": "2025-10-05T06:32:26.536138Z",
     "shell.execute_reply.started": "2025-10-05T06:11:01.336840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== Setup & Imports ==================\n",
    "# !pip -q install transformers==4.42.4 accelerate datasets==2.21.0\n",
    "\n",
    "import os, random, math, json, numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaModel, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ------------------- Repro -------------------\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ================== Paths (your config) ==================\n",
    "TXT_PATHS = {\n",
    "    'train': '/kaggle/working/UMT/data/twitter2015/train.txt',\n",
    "    'val':   '/kaggle/working/UMT/data/twitter2015/valid.txt',\n",
    "    'test':  '/kaggle/working/UMT/data/twitter2015/test.txt'\n",
    "}\n",
    "IMG_DIRS = {\n",
    "    'train': '/kaggle/input/twitter2015/twitter2015/twitter2015_images',\n",
    "    'val':   '/kaggle/input/twitter2015/twitter2015/twitter2015_images',\n",
    "    'test':  '/kaggle/input/twitter2015/twitter2015/twitter2015_images'\n",
    "}\n",
    "\n",
    "# ================== Utils ==================\n",
    "def parse_twitter_conll(path: str):\n",
    "    \"\"\"Parse Twitter2015 style: blocks separated by blank lines, starting with IMGID:xxxx\"\"\"\n",
    "    samples = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [ln.rstrip('\\n') for ln in f]\n",
    "    img_id, toks, tags = None, [], []\n",
    "    for ln in lines + ['']:  # sentinel blank to flush last sample\n",
    "        if not ln.strip():\n",
    "            if img_id is not None and toks:\n",
    "                samples.append({'img_id': img_id, 'tokens': toks, 'labels': tags})\n",
    "            img_id, toks, tags = None, [], []\n",
    "            continue\n",
    "        if ln.startswith('IMGID:'):\n",
    "            img_id = ln.split(':', 1)[1].strip()\n",
    "        else:\n",
    "            # token and BIO tag separated by whitespace\n",
    "            parts = ln.split()\n",
    "            if len(parts) >= 2:\n",
    "                toks.append(parts[0])\n",
    "                tags.append(parts[1])\n",
    "    return samples\n",
    "\n",
    "def build_label_vocab(*lists_of_samples):\n",
    "    labels = set()\n",
    "    for s_list in lists_of_samples:\n",
    "        for s in s_list:\n",
    "            labels.update(s['labels'])\n",
    "    labels = sorted(labels)  # stable order\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "    id2label = {i:l for l,i in label2id.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "def find_image_path(img_dir: str, img_id: str):\n",
    "    for ext in ('.jpg', '.jpeg', '.png', '.bmp'):\n",
    "        p = os.path.join(img_dir, img_id + ext)\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# ================== Dataset ==================\n",
    "class Twitter2015MNER(Dataset):\n",
    "    def __init__(self, samples, img_dir, tokenizer: RobertaTokenizerFast, label2id, max_len=128, aug=False):\n",
    "        self.samples = samples\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if aug:\n",
    "            self.img_tfm = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomResizedCrop((224,224), scale=(0.9,1.0)),\n",
    "                transforms.ColorJitter(0.15,0.15,0.15,0.05),\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            self.img_tfm = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "            ])\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.samples[idx]\n",
    "        tokens: List[str] = ex['tokens']\n",
    "        labels: List[str] = ex['labels']\n",
    "\n",
    "        # Tokenize with word alignment\n",
    "        # --- Tokenize text first ---\n",
    "        encodings = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 👉 Call .word_ids BEFORE squeezing\n",
    "        word_ids = encodings.word_ids(batch_index=0)\n",
    "        \n",
    "        # Then convert tensors for PyTorch\n",
    "        enc = {k: v.squeeze(0) for k, v in encodings.items()}\n",
    "        \n",
    "        # --- Align labels with subwords ---\n",
    "        label_ids = []\n",
    "        prev_word = None\n",
    "        for w_id in word_ids:\n",
    "            if w_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif w_id != prev_word:\n",
    "                label_ids.append(self.label2id[labels[w_id]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            prev_word = w_id\n",
    "        label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "\n",
    "        # Image\n",
    "        img_path = find_image_path(self.img_dir, ex['img_id'])\n",
    "        if img_path is None:\n",
    "            # fallback: blank image if missing\n",
    "            img = Image.new('RGB', (224,224), color=(0,0,0))\n",
    "        else:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        img = self.img_tfm(img)\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'],\n",
    "            'attention_mask': enc['attention_mask'],\n",
    "            'pixel_values': img,\n",
    "            'labels': label_ids\n",
    "        }\n",
    "\n",
    "# ================== Model: RoBERTa-large + ResNet50 (FiLM-style fusion) ==================\n",
    "class RobertaResNet50MNER(nn.Module):\n",
    "    \"\"\"\n",
    "    Token classification with image-conditioned modulation (FiLM-like):\n",
    "      - Text encoder: roberta-large (hidden=1024)\n",
    "      - Image encoder: resnet50 -> 2048-d pooled -> Linear -> 1024\n",
    "      - gamma = Wg(img), beta = Wb(img)\n",
    "      - h' = (1 + gamma) * h + beta  (applied to every token)\n",
    "      - Token classifier to BIO tag space\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels, text_model='roberta-large'):\n",
    "        super().__init__()\n",
    "        self.text = RobertaModel.from_pretrained(text_model)\n",
    "        hidden = self.text.config.hidden_size  # 1024\n",
    "\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.visual = resnet\n",
    "\n",
    "        self.img_proj = nn.Linear(2048, hidden)\n",
    "        self.gamma = nn.Linear(hidden, hidden)\n",
    "        self.beta  = nn.Linear(hidden, hidden)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n",
    "        # Text\n",
    "        out = self.text(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq = out.last_hidden_state  # [B, L, 1024]\n",
    "\n",
    "        # Image\n",
    "        img_feat = self.visual(pixel_values)       # [B, 2048]\n",
    "        img_feat = self.img_proj(img_feat)         # [B, 1024]\n",
    "\n",
    "        # FiLM modulation per token\n",
    "        g = self.gamma(img_feat).unsqueeze(1)      # [B, 1, 1024]\n",
    "        b = self.beta(img_feat).unsqueeze(1)       # [B, 1, 1024]\n",
    "        seq = (1 + g) * seq + b                    # [B, L, 1024]\n",
    "\n",
    "        seq = self.dropout(seq)\n",
    "        logits = self.classifier(seq)              # [B, L, C]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "# ================== Build Data ==================\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large', add_prefix_space=True)\n",
    "\n",
    "\n",
    "train_samples = parse_twitter_conll(TXT_PATHS['train'])\n",
    "val_samples   = parse_twitter_conll(TXT_PATHS['val'])\n",
    "test_samples  = parse_twitter_conll(TXT_PATHS['test'])\n",
    "\n",
    "label2id, id2label = build_label_vocab(train_samples, val_samples, test_samples)\n",
    "print(\"Labels:\", label2id)\n",
    "\n",
    "train_ds = Twitter2015MNER(train_samples, IMG_DIRS['train'], tokenizer, label2id, max_len=128, aug=True)\n",
    "val_ds   = Twitter2015MNER(val_samples,   IMG_DIRS['val'],   tokenizer, label2id, max_len=128, aug=False)\n",
    "test_ds  = Twitter2015MNER(test_samples,  IMG_DIRS['test'],  tokenizer, label2id, max_len=128, aug=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=12, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=12, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ================== Train ==================\n",
    "model = RobertaResNet50MNER(num_labels=len(label2id)).to(device)\n",
    "\n",
    "EPOCHS = 5\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup = int(0.1 * total_steps)\n",
    "sched = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup, num_training_steps=total_steps)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train: model.train()\n",
    "    else: model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    losses = []\n",
    "\n",
    "    it = tqdm(loader, leave=False, desc=\"Train\" if train else \"Eval\")\n",
    "    for batch in it:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn      = batch['attention_mask'].to(device)\n",
    "        pixels    = batch['pixel_values'].to(device)\n",
    "        labels    = batch['labels'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            logits, loss = model(input_ids, attn, pixels, labels if train else labels)\n",
    "\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optim.step()\n",
    "            sched.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # collect predictions & labels for metrics (ignore -100)\n",
    "        preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        golds = labels.detach().cpu().numpy()\n",
    "\n",
    "        for p, g in zip(preds, golds):\n",
    "            for pi, gi in zip(p, g):\n",
    "                if gi == -100:  # skip subwords / pads\n",
    "                    continue\n",
    "                all_preds.append(pi)\n",
    "                all_labels.append(gi)\n",
    "\n",
    "    avg_loss = float(np.mean(losses))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    return avg_loss, acc, prec, rec, f1\n",
    "\n",
    "best_val_f1, best_state = -1.0, None\n",
    "patience, bad = 2, 0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc, tr_p, tr_r, tr_f1 = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_acc, vl_p, vl_r, vl_f1 = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train loss {tr_loss:.4f} acc {tr_acc:.3f} P {tr_p:.3f} R {tr_r:.3f} F1 {tr_f1:.3f} || \"\n",
    "          f\"Val loss {vl_loss:.4f} acc {vl_acc:.3f} P {vl_p:.3f} R {vl_r:.3f} F1 {vl_f1:.3f}\")\n",
    "\n",
    "    if vl_f1 > best_val_f1:\n",
    "        best_val_f1 = vl_f1; bad = 0\n",
    "        best_state = {\n",
    "            'model': model.state_dict(),\n",
    "            'label2id': label2id,\n",
    "            'id2label': id2label,\n",
    "            'config': {'text':'roberta-large','img':'resnet50','fusion':'FiLM'}\n",
    "        }\n",
    "        torch.save(best_state, 'roberta_resnet50_mner_best.pth')\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# ================== Evaluate on Test ==================\n",
    "# load best\n",
    "if best_state is None:\n",
    "    best_state = torch.load('roberta_resnet50_mner_best.pth', map_location=device)\n",
    "model.load_state_dict(best_state['model'])\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, leave=False, desc=\"Test\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn      = batch['attention_mask'].to(device)\n",
    "            pixels    = batch['pixel_values'].to(device)\n",
    "            labels    = batch['labels'].to(device)\n",
    "            logits, loss = model(input_ids, attn, pixels, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            golds = labels.detach().cpu().numpy()\n",
    "            for p, g in zip(preds, golds):\n",
    "                for pi, gi in zip(p, g):\n",
    "                    if gi == -100: continue\n",
    "                    all_preds.append(pi); all_labels.append(gi)\n",
    "\n",
    "    avg_loss = float(np.mean(losses))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    return avg_loss, acc, prec, rec, f1\n",
    "\n",
    "te_loss, te_acc, te_p, te_r, te_f1 = evaluate(test_loader)\n",
    "print(\"\\n===== TEST RESULTS (Token-level, ignore subwords) =====\")\n",
    "print(f\"Loss: {te_loss:.4f}\")\n",
    "print(f\"Accuracy:  {te_acc:.4f}\")\n",
    "print(f\"Precision: {te_p:.4f}\")\n",
    "print(f\"Recall:    {te_r:.4f}\")\n",
    "print(f\"F1:        {te_f1:.4f}\")\n",
    "\n",
    "# (Optional) per-label report\n",
    "def per_label_report(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn      = batch['attention_mask'].to(device)\n",
    "            pixels    = batch['pixel_values'].to(device)\n",
    "            labels    = batch['labels'].to(device)\n",
    "            logits, _ = model(input_ids, attn, pixels, labels)\n",
    "            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "            golds = labels.detach().cpu().numpy()\n",
    "            for p, g in zip(preds, golds):\n",
    "                for pi, gi in zip(p, g):\n",
    "                    if gi == -100: continue\n",
    "                    all_preds.append(pi); all_labels.append(gi)\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    target_names = [l for i,l in sorted(id2label.items())]\n",
    "    print(\"\\nPer-label report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, zero_division=0))\n",
    "\n",
    "# Uncomment to print per-label metrics:\n",
    "# per_label_report(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate our models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T06:44:41.082050Z",
     "iopub.status.busy": "2025-10-05T06:44:41.081230Z",
     "iopub.status.idle": "2025-10-05T06:45:27.745453Z",
     "shell.execute_reply": "2025-10-05T06:45:27.744525Z",
     "shell.execute_reply.started": "2025-10-05T06:44:41.082018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_labels, all_preds = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn      = batch['attention_mask'].to(device)\n",
    "        pixels    = batch['pixel_values'].to(device)\n",
    "        labels    = batch['labels'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attn, pixels, labels)\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        golds = labels.cpu().numpy()\n",
    "\n",
    "        # collect only valid tokens (ignore subwords/pads)\n",
    "        for p, g in zip(preds, golds):\n",
    "            for pi, gi in zip(p, g):\n",
    "                if gi == -100:\n",
    "                    continue\n",
    "                all_preds.append(pi)\n",
    "                all_labels.append(gi)\n",
    "\n",
    "# ---------------- Overall Metrics ----------------\n",
    "acc  = accuracy_score(all_labels, all_preds)\n",
    "prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro')\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(\"\\n===== Detailed Test Results =====\")\n",
    "print(f\"Accuracy:        {acc:.4f}\")\n",
    "print(f\"Micro F1:        {f1_micro:.4f}\")\n",
    "print(f\"Macro F1:        {f1_macro:.4f}\")\n",
    "print(f\"Weighted F1:     {f1_weighted:.4f}\")\n",
    "print(f\"Micro Precision: {prec_micro:.4f}\")\n",
    "print(f\"Micro Recall:    {rec_micro:.4f}\")\n",
    "\n",
    "# ---------------- Per-label report ----------------\n",
    "target_names = [id2label[i] for i in range(len(id2label))]\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=target_names,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    ")\n",
    "print(\"\\nPer-label classification report:\")\n",
    "print(report)\n",
    "\n",
    "# ---------------- Confusion Matrix ----------------\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(id2label))))\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(\n",
    "    cm, annot=False, cmap='YlGnBu',\n",
    "    xticklabels=target_names, yticklabels=target_names\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix – Token Level\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T06:49:09.640450Z",
     "iopub.status.busy": "2025-10-05T06:49:09.640187Z",
     "iopub.status.idle": "2025-10-05T06:49:13.022535Z",
     "shell.execute_reply": "2025-10-05T06:49:13.021821Z",
     "shell.execute_reply.started": "2025-10-05T06:49:09.640432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import RobertaTokenizerFast\n",
    "from torchvision import transforms\n",
    "\n",
    "# === Load tokenizer and model ===\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-large\", add_prefix_space=True)\n",
    "model = RobertaResNet50MNER(num_labels=len(id2label))\n",
    "ckpt = torch.load(\"roberta_resnet50_mner_best.pth\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Image preprocessing (same as training) ===\n",
    "img_tfm = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# === Example: choose one image ID from Twitter2015 ===\n",
    "img_id = \"74960\"   # change this to test others\n",
    "img_path = f\"/kaggle/input/twitter2015/twitter2015/twitter2015_images/{img_id}.jpg\"\n",
    "\n",
    "tweet_text = \"RT @JayKenMinaj : Me outside of where George Zimmerman got shot at . You know God is so good .\"\n",
    "\n",
    "# --- Load and preprocess ---\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "pixel_values = img_tfm(image).unsqueeze(0).to(device)\n",
    "\n",
    "# --- Tokenize ---\n",
    "tokens = tweet_text.split()\n",
    "enc = tokenizer(tokens, is_split_into_words=True, padding='max_length',\n",
    "                truncation=True, max_length=128, return_tensors='pt')\n",
    "input_ids = enc[\"input_ids\"].to(device)\n",
    "attn = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "# --- Inference ---\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(input_ids, attn, pixel_values)\n",
    "preds = torch.argmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "# --- Decode predicted tags ---\n",
    "word_ids = enc.word_ids(batch_index=0)\n",
    "pred_tags = []\n",
    "prev_word = None\n",
    "for i, w_id in enumerate(word_ids):\n",
    "    if w_id is None or w_id == prev_word:\n",
    "        continue\n",
    "    tag = id2label[int(preds[i])]\n",
    "    pred_tags.append((tokens[w_id], tag))\n",
    "    prev_word = w_id\n",
    "\n",
    "# === Print results ===\n",
    "print(f\"Predicted entities for IMGID {img_id}:\")\n",
    "for tok, tag in pred_tags:\n",
    "    if tag != \"O\":\n",
    "        print(f\"{tok:15s} → {tag}\")\n",
    "\n",
    "# --- visualize ---\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Predicted entities for IMGID {img_id}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6075740,
     "sourceId": 9892595,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6076847,
     "sourceId": 9894002,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
