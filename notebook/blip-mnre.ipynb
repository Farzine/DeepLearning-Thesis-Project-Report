{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T13:15:13.980741Z",
     "iopub.status.busy": "2025-09-27T13:15:13.980476Z",
     "iopub.status.idle": "2025-09-27T13:15:38.072112Z",
     "shell.execute_reply": "2025-09-27T13:15:38.071276Z",
     "shell.execute_reply.started": "2025-09-27T13:15:13.980707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T13:15:38.074142Z",
     "iopub.status.busy": "2025-09-27T13:15:38.073588Z",
     "iopub.status.idle": "2025-09-27T13:16:42.772664Z",
     "shell.execute_reply": "2025-09-27T13:16:42.771372Z",
     "shell.execute_reply.started": "2025-09-27T13:15:38.074121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/thecharm/MNRE.git\n",
    "!pip install -q gdown\n",
    "file_id = \"1FYiJFtRayWY32nRH0rdycYzIdDcMmDFR\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", quiet=False)\n",
    "!unzip mnre_img.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T13:16:42.774143Z",
     "iopub.status.busy": "2025-09-27T13:16:42.773859Z",
     "iopub.status.idle": "2025-09-27T13:16:42.778994Z",
     "shell.execute_reply": "2025-09-27T13:16:42.778181Z",
     "shell.execute_reply.started": "2025-09-27T13:16:42.774113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "txt_paths = {\n",
    "        'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "        'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val.txt',  # fixed space typo\n",
    "        'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt'\n",
    "    }\n",
    "img_dirs = {\n",
    "        'train': '/kaggle/working/img_org/train',\n",
    "        'val':   '/kaggle/working/img_org/val',\n",
    "        'test':  '/kaggle/working/img_org/test'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T13:16:42.780362Z",
     "iopub.status.busy": "2025-09-27T13:16:42.780082Z",
     "iopub.status.idle": "2025-09-27T14:04:47.615018Z",
     "shell.execute_reply": "2025-09-27T14:04:47.614139Z",
     "shell.execute_reply.started": "2025-09-27T13:16:42.780343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import BlipProcessor, BlipModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# 1. Load BLIP (pretrained vision-language model)\n",
    "# ========================\n",
    "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
    "blip_model = BlipModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Freeze BLIP encoder (optional)\n",
    "for param in blip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "HIDDEN_SIZE = blip_model.config.text_config.hidden_size  # 768\n",
    "\n",
    "# ========================\n",
    "# 2. Dataset\n",
    "# ========================\n",
    "class MNREDataset(Dataset):\n",
    "    def __init__(self, txt_file, img_dir, relation2id, transform=None, max_len=128):\n",
    "        self.samples = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                tokens = obj['token']\n",
    "                h_start, h_end = obj['h']['pos']\n",
    "                t_start, t_end = obj['t']['pos']\n",
    "\n",
    "                # Insert entity markers\n",
    "                spans = sorted([('h', h_start, h_end), ('t', t_start, t_end)], key=lambda x: x[1], reverse=True)\n",
    "                for etype, start, end in spans:\n",
    "                    tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "                    tokens.insert(end, tag_close)\n",
    "                    tag_open = '[E1]' if etype == 'h' else '[E2]'\n",
    "                    tokens.insert(start, tag_open)\n",
    "\n",
    "                text = \" \".join(tokens)\n",
    "                img_id = obj['img_id']\n",
    "                label = relation2id[obj['relation']]\n",
    "\n",
    "                self.samples.append({\"text\": text, \"img_id\": img_id, \"label\": label})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_path = os.path.join(self.img_dir, sample['img_id'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = processor(images=image, text=sample['text'], padding=\"max_length\", truncation=True,\n",
    "                           max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# 3. Model (Relation Classifier)\n",
    "# ========================\n",
    "class MultimodalREModel(nn.Module):\n",
    "    def __init__(self, num_relations):\n",
    "        super().__init__()\n",
    "        self.blip = blip_model\n",
    "\n",
    "        # ğŸ”¹ Get actual BLIP hidden size (can be 512 or 768 depending on model variant)\n",
    "        hidden_size = self.blip.config.projection_dim  \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_relations)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        outputs = self.blip(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            pixel_values=pixel_values)\n",
    "\n",
    "        pooled = outputs.image_embeds  # or outputs.text_embeds (both are projection_dim size)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 4. Training & Evaluation\n",
    "# ========================\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    losses, preds, labels = [], [], []\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch[\"attention_mask\"].to(device)\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        labs = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attn, pixels)\n",
    "        loss = criterion(logits, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return sum(losses)/len(losses), accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, preds, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            pixels = batch[\"pixel_values\"].to(device)\n",
    "            labs = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attn, pixels)\n",
    "            loss = criterion(logits, labs)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "            labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return sum(losses)/len(losses), accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "# ========================\n",
    "# 5. Main\n",
    "# ========================\n",
    "def main():\n",
    "    txt_paths = {\n",
    "        'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "        'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val .txt',\n",
    "        'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt'\n",
    "    }\n",
    "    img_dirs = {\n",
    "        'train': '/kaggle/working/img_org/train',\n",
    "        'val':   '/kaggle/working/img_org/val',\n",
    "        'test':  '/kaggle/working/img_org/test'\n",
    "    }\n",
    "\n",
    "    # Build relation mapping\n",
    "    rels = set()\n",
    "    with open(txt_paths[\"train\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj[\"relation\"])\n",
    "    relation2id = {r: idx for idx, r in enumerate(sorted(rels))}\n",
    "    num_relations = len(relation2id)\n",
    "\n",
    "    # Datasets & Loaders\n",
    "    datasets = {\n",
    "        split: MNREDataset(txt_paths[split], img_dirs[split], relation2id)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    loaders = {\n",
    "        split: DataLoader(datasets[split], batch_size=8, shuffle=(split==\"train\"))\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = MultimodalREModel(num_relations).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    EPOCHS = 5\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc, tr_f1 = train_epoch(model, loaders[\"train\"], criterion, optimizer)\n",
    "        val_loss, val_acc, val_f1 = eval_epoch(model, loaders[\"val\"], criterion)\n",
    "\n",
    "        history[\"train\"].append((tr_acc, tr_f1))\n",
    "        history[\"val\"].append((val_acc, val_f1))\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | Train Acc: {tr_acc:.4f} F1: {tr_f1:.4f} | Val Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Test Evaluation\n",
    "    test_loss, test_acc, test_f1 = eval_epoch(model, loaders[\"test\"], criterion)\n",
    "    print(f\"âœ… Test Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return model, relation2id\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, relation2id = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T14:10:35.511544Z",
     "iopub.status.busy": "2025-09-27T14:10:35.511225Z",
     "iopub.status.idle": "2025-09-27T14:10:51.268578Z",
     "shell.execute_reply": "2025-09-27T14:10:51.267917Z",
     "shell.execute_reply.started": "2025-09-27T14:10:35.511524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BlipProcessor, BlipModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BLIP backbone (pretrained multimodal)\n",
    "BLIP_NAME = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "# Your MNRE files (adjust if needed)\n",
    "TXT_PATHS = {\n",
    "    'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "    'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val.txt',   # ensure no trailing space in filename\n",
    "    'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt',\n",
    "}\n",
    "IMG_DIRS = {\n",
    "    'train': '/kaggle/working/img_org/train',\n",
    "    'val':   '/kaggle/working/img_org/val',\n",
    "    'test':  '/kaggle/working/img_org/test',\n",
    "}\n",
    "\n",
    "# Your trained RE checkpoint (produced by the BLIP training script)\n",
    "CKPT_PATH = \"mnre_blip_re.pth\"  # change if your filename differs\n",
    "\n",
    "# Example images the user gave\n",
    "IMAGES_TO_PREDICT = [\n",
    "    \"/kaggle/working/img_org/test/twitter_19_31_0_13.jpg\",\n",
    "    \"/kaggle/working/img_org/test/twitter_19_31_0_8.jpg\",\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def safe_fix_val_path():\n",
    "    \"\"\"If the val file was created with an accidental space, rename it once.\"\"\"\n",
    "    bad = \"/kaggle/working/MNRE/mnre_txt/mnre_val .txt\"\n",
    "    good = \"/kaggle/working/MNRE/mnre_txt/mnre_val.txt\"\n",
    "    if os.path.exists(bad) and not os.path.exists(good):\n",
    "        os.rename(bad, good)\n",
    "\n",
    "def load_all_mnre_entries(txt_paths):\n",
    "    \"\"\"Load ALL entries across train/val/test for lookup by img_id.\"\"\"\n",
    "    entries = []\n",
    "    for split, p in txt_paths.items():\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing file for split '{split}': {p}\")\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                obj[\"_split\"] = split\n",
    "                entries.append(obj)\n",
    "    return entries\n",
    "\n",
    "def build_relation2id_from_train(train_path):\n",
    "    \"\"\"Build relation2id by scanning the train file (consistent with training).\"\"\"\n",
    "    rels = set()\n",
    "    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj[\"relation\"])\n",
    "    relation2id = {r: idx for idx, r in enumerate(sorted(rels))}\n",
    "    return relation2id\n",
    "\n",
    "def insert_markers(tokens, h_pos, t_pos):\n",
    "    \"\"\"Insert [E1] [/E1], [E2] [/E2] into tokens given head/tail spans.\"\"\"\n",
    "    toks = tokens.copy()\n",
    "    spans = [('h', h_pos[0], h_pos[1]), ('t', t_pos[0], t_pos[1])]\n",
    "    spans.sort(key=lambda x: x[1], reverse=True)\n",
    "    for etype, start, end in spans:\n",
    "        tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "        tag_open  = '[E1]'  if etype == 'h' else '[E2]'\n",
    "        toks.insert(end, tag_close)\n",
    "        toks.insert(start, tag_open)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def extract_entities_from_marked_text(marked_text):\n",
    "    \"\"\"Return (head_text, tail_text) by parsing [E1]..[/E1] and [E2]..[/E2].\"\"\"\n",
    "    def between(text, open_tag, close_tag):\n",
    "        if open_tag in text and close_tag in text:\n",
    "            s = text.index(open_tag) + len(open_tag)\n",
    "            e = text.index(close_tag)\n",
    "            return text[s:e].strip()\n",
    "        return None\n",
    "    h_text = between(marked_text, \"[E1]\", \"[/E1]\")\n",
    "    t_text = between(marked_text, \"[E2]\", \"[/E2]\")\n",
    "    return h_text, t_text\n",
    "\n",
    "# ----------------------------\n",
    "# BLIP-based RE model definition (same as training)\n",
    "# ----------------------------\n",
    "class BLIPRelationClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification on top of BLIP's projection embeddings.\n",
    "    We use blip.config.projection_dim to stay compatible across variants.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_relations, blip_model):\n",
    "        super().__init__()\n",
    "        self.blip = blip_model\n",
    "        hidden = self.blip.config.projection_dim  # 512 for base captioning model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_relations)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, use_text_embeds=True):\n",
    "        \"\"\"\n",
    "        use_text_embeds=True:\n",
    "            classify using BLIP's text_embeds (projected text representation)\n",
    "        use_text_embeds=False:\n",
    "            classify using BLIP's image_embeds\n",
    "        Optionally, you could concatenate both.\n",
    "        \"\"\"\n",
    "        out = self.blip(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        pixel_values=pixel_values)\n",
    "        pooled = out.text_embeds if use_text_embeds else out.image_embeds\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Load BLIP + Processor + NER pipeline\n",
    "# ----------------------------\n",
    "def load_backbones():\n",
    "    processor = BlipProcessor.from_pretrained(BLIP_NAME)\n",
    "    blip_model = BlipModel.from_pretrained(BLIP_NAME).to(device)\n",
    "    # We typically freeze BLIP for inference; it doesn't matter here.\n",
    "    for p in blip_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # NER model (pretrained token-classification)\n",
    "    ner_name = \"dslim/bert-base-NER\"\n",
    "    ner_tok = AutoTokenizer.from_pretrained(ner_name)\n",
    "    ner_mdl = AutoModelForTokenClassification.from_pretrained(ner_name).to(device)\n",
    "    ner_pipe = pipeline(\"ner\", model=ner_mdl, tokenizer=ner_tok, aggregation_strategy=\"simple\", device=0 if device.type==\"cuda\" else -1)\n",
    "\n",
    "    return processor, blip_model, ner_pipe\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare model from checkpoint (or fall back)\n",
    "# ----------------------------\n",
    "def prepare_re_model(processor, blip_model, relation2id, ckpt_path):\n",
    "    model = BLIPRelationClassifier(num_relations=len(relation2id), blip_model=blip_model).to(device)\n",
    "    if os.path.exists(ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        missing = model.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
    "        if missing.missing_keys or missing.unexpected_keys:\n",
    "            print(\"âš ï¸ State dict mismatch:\", missing)\n",
    "        id2rel_from_ckpt = {v:k for k,v in ckpt.get(\"relation2id\", {}).items()}\n",
    "        print(f\"âœ… Loaded RE checkpoint: {ckpt_path}\")\n",
    "        return model, id2rel_from_ckpt if id2rel_from_ckpt else {v:k for k,v in relation2id.items()}\n",
    "    else:\n",
    "        print(f\"âš ï¸ RE checkpoint not found at {ckpt_path}. Using fresh classifier weights (accuracy will be poor).\")\n",
    "        return model, {v:k for k,v in relation2id.items()}\n",
    "\n",
    "# ----------------------------\n",
    "# Build an index: img_id -> list of entries\n",
    "# ----------------------------\n",
    "def index_entries_by_img_id(entries):\n",
    "    idx = {}\n",
    "    for e in entries:\n",
    "        img_id = e[\"img_id\"]\n",
    "        idx.setdefault(img_id, []).append(e)\n",
    "    return idx\n",
    "\n",
    "# ----------------------------\n",
    "# Single-sample preprocessing for BLIP\n",
    "# ----------------------------\n",
    "def build_blip_inputs(processor, image_path, marked_text, max_len=128):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, text=marked_text,\n",
    "                       padding=\"max_length\", truncation=True,\n",
    "                       max_length=max_len, return_tensors=\"pt\")\n",
    "    # Move to device\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    return image, inputs\n",
    "\n",
    "# ----------------------------\n",
    "# Pretty print + visualize\n",
    "# ----------------------------\n",
    "def show_result(image, pred_relation, confidence, sentence, h_text, h_type, t_text, t_type):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Relation: {pred_relation} ({confidence:.1%})\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(\"ğŸ“ Sentence:\")\n",
    "    print(sentence)\n",
    "    print(\"\\nğŸ‘¤ Head Entity (E1):\", h_text, \"| NER type:\", h_type)\n",
    "    print(\"ğŸ‘¥ Tail Entity (E2):\", t_text, \"| NER type:\", t_type)\n",
    "    print(\"ğŸ”— Predicted Relation:\", pred_relation, f\"(conf={confidence:.4f})\")\n",
    "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# NER helper for entity types\n",
    "# ----------------------------\n",
    "def ner_type_for_span(ner_pipe, full_text, span_text):\n",
    "    \"\"\"\n",
    "    Run NER on the full sentence; pick the dominant label covering the given span_text.\n",
    "    If not found, return 'MISC'.\n",
    "    \"\"\"\n",
    "    ents = ner_pipe(full_text)\n",
    "    # Find any entity whose text overlaps with span_text (simple contains)\n",
    "    candidates = [e for e in ents if span_text and span_text in e[\"word\"] or span_text and span_text in e.get(\"entity_group\",\"\")]\n",
    "    if not candidates:\n",
    "        # looser matching: check if entity words are inside the span text\n",
    "        for e in ents:\n",
    "            if e[\"word\"] and e[\"word\"] in span_text:\n",
    "                candidates.append(e)\n",
    "    if not candidates:\n",
    "        return \"MISC\"\n",
    "    # majority label\n",
    "    from collections import Counter\n",
    "    label = Counter([e[\"entity_group\"] for e in candidates]).most_common(1)[0][0]\n",
    "    return label\n",
    "\n",
    "# ----------------------------\n",
    "# Main: predict on a list of image paths\n",
    "# ----------------------------\n",
    "def predict_images(image_paths):\n",
    "    # Fix any bad filename first\n",
    "    safe_fix_val_path()\n",
    "\n",
    "    # Load entries and index by img_id\n",
    "    entries = load_all_mnre_entries(TXT_PATHS)\n",
    "    by_img = index_entries_by_img_id(entries)\n",
    "\n",
    "    # Build relation2id (from train) for consistent id order\n",
    "    relation2id_train = build_relation2id_from_train(TXT_PATHS[\"train\"])\n",
    "\n",
    "    # Backbones\n",
    "    processor, blip_model, ner_pipe = load_backbones()\n",
    "\n",
    "    # RE model\n",
    "    re_model, id2relation = prepare_re_model(processor, blip_model, relation2id_train, CKPT_PATH)\n",
    "    re_model.eval()\n",
    "\n",
    "    # Go over requested images\n",
    "    for ipath in image_paths:\n",
    "        if not os.path.exists(ipath):\n",
    "            print(f\"âŒ Image not found: {ipath}\")\n",
    "            continue\n",
    "        img_id = os.path.basename(ipath)\n",
    "\n",
    "        if img_id not in by_img:\n",
    "            print(f\"âš ï¸ No MNRE entry found for image id: {img_id}\")\n",
    "            # Still show the image\n",
    "            image = Image.open(ipath).convert(\"RGB\")\n",
    "            plt.imshow(image); plt.axis('off'); plt.title(\"Image (no matching text entry)\"); plt.show()\n",
    "            continue\n",
    "\n",
    "        # MNRE can have multiple sentences per image; iterate all\n",
    "        for sample in by_img[img_id]:\n",
    "            tokens = sample[\"token\"]\n",
    "            h_pos = sample[\"h\"][\"pos\"]\n",
    "            t_pos = sample[\"t\"][\"pos\"]\n",
    "\n",
    "            # Insert entity markers so model knows E1/E2\n",
    "            marked_text = insert_markers(tokens, h_pos, t_pos)\n",
    "            h_text, t_text = extract_entities_from_marked_text(marked_text)\n",
    "\n",
    "            # Build BLIP inputs\n",
    "            image, inputs = build_blip_inputs(processor, ipath, marked_text, max_len=128)\n",
    "\n",
    "            # RE prediction\n",
    "            with torch.no_grad():\n",
    "                logits = re_model(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"pixel_values\"], use_text_embeds=True)\n",
    "                probs  = torch.softmax(logits, dim=1)\n",
    "                pred_i = int(torch.argmax(probs, dim=1).item())\n",
    "                conf   = float(probs[0, pred_i].item())\n",
    "                pred_relation = id2relation.get(pred_i, f\"rel_{pred_i}\")\n",
    "\n",
    "            # NER types for E1/E2 (using pretrained NER)\n",
    "            # We run NER on the sentence WITHOUT tags for better tagging\n",
    "            clean_sentence = marked_text.replace(\"[E1]\", \"\").replace(\"[/E1]\", \"\").replace(\"[E2]\", \"\").replace(\"[/E2]\", \"\")\n",
    "            h_type = ner_type_for_span(ner_pipe, clean_sentence, h_text or \"\")\n",
    "            t_type = ner_type_for_span(ner_pipe, clean_sentence, t_text or \"\")\n",
    "\n",
    "            # Display\n",
    "            show_result(image, pred_relation, conf, marked_text, h_text, h_type, t_text, t_type)\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "predict_images(IMAGES_TO_PREDICT)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8220783,
     "sourceId": 12987908,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
