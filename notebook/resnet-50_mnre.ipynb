{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:05:44.547877Z",
     "iopub.status.busy": "2025-10-14T06:05:44.547554Z",
     "iopub.status.idle": "2025-10-14T06:05:44.566093Z",
     "shell.execute_reply": "2025-10-14T06:05:44.565447Z",
     "shell.execute_reply.started": "2025-10-14T06:05:44.547852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:05:44.567408Z",
     "iopub.status.busy": "2025-10-14T06:05:44.567182Z",
     "iopub.status.idle": "2025-10-14T06:05:45.323890Z",
     "shell.execute_reply": "2025-10-14T06:05:45.323270Z",
     "shell.execute_reply.started": "2025-10-14T06:05:44.567374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def clear_kaggle_cache():\n",
    "    \"\"\"\n",
    "    Clear all possible caches and temporary files from Kaggle's working directories.\n",
    "    This will free up disk space by removing unnecessary files.\n",
    "    \"\"\"\n",
    "    cache_dirs = [\n",
    "        \"/kaggle/working\",  # Working directory for models\n",
    "    ]\n",
    "    \n",
    "    for cache_dir in cache_dirs:\n",
    "        if os.path.exists(cache_dir):\n",
    "            try:\n",
    "                # Remove all files and subdirectories in the cache\n",
    "                for filename in os.listdir(cache_dir):\n",
    "                    file_path = os.path.join(cache_dir, filename)\n",
    "                    try:\n",
    "                        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                            os.unlink(file_path)\n",
    "                        elif os.path.isdir(file_path):\n",
    "                            shutil.rmtree(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Could not delete {file_path}: {e}\")\n",
    "                print(f\"üßπ Emptied: {cache_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not clear {cache_dir}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Directory does not exist: {cache_dir}\")\n",
    "\n",
    "# Call the function to clean up\n",
    "clear_kaggle_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:05:45.324717Z",
     "iopub.status.busy": "2025-10-14T06:05:45.324477Z",
     "iopub.status.idle": "2025-10-14T06:06:09.343136Z",
     "shell.execute_reply": "2025-10-14T06:06:09.342088Z",
     "shell.execute_reply.started": "2025-10-14T06:05:45.324700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/thecharm/MNRE.git\n",
    "!pip install -q gdown\n",
    "file_id = \"1FYiJFtRayWY32nRH0rdycYzIdDcMmDFR\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", quiet=False)\n",
    "!unzip mnre_img.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:06:09.345636Z",
     "iopub.status.busy": "2025-10-14T06:06:09.345376Z",
     "iopub.status.idle": "2025-10-14T06:06:09.354199Z",
     "shell.execute_reply": "2025-10-14T06:06:09.353463Z",
     "shell.execute_reply.started": "2025-10-14T06:06:09.345616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, ast, math, json, random\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
    "\n",
    "# -------------- Repro --------------\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------- Paths (EDIT if needed) --------------\n",
    "TXT_PATHS = {\n",
    "    'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "    'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val .txt',   # will auto-fix if 'mnre_val .txt' exists\n",
    "    'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt'\n",
    "}\n",
    "IMG_DIRS = {\n",
    "    'train': '/kaggle/working/img_org/train',\n",
    "    'val':   '/kaggle/working/img_org/val',\n",
    "    'test':  '/kaggle/working/img_org/test'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visulization and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:06:09.355234Z",
     "iopub.status.busy": "2025-10-14T06:06:09.354998Z",
     "iopub.status.idle": "2025-10-14T06:06:11.653703Z",
     "shell.execute_reply": "2025-10-14T06:06:11.652861Z",
     "shell.execute_reply.started": "2025-10-14T06:06:09.355210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "# --------------------\n",
    "# 1. Load Data and Relation Distribution\n",
    "# --------------------\n",
    "# Helper function to load data from MNRE text files\n",
    "def load_data(txt_path):\n",
    "    data = []\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "train_data = load_data(TXT_PATHS['train'])\n",
    "val_data = load_data(TXT_PATHS['val'])\n",
    "test_data = load_data(TXT_PATHS['test'])\n",
    "\n",
    "# --------------------\n",
    "# 2. Relation Distribution\n",
    "# --------------------\n",
    "relations = [obj['relation'] for obj in train_data]\n",
    "relation_counts = dict(Counter(relations))\n",
    "\n",
    "# Plot distribution of relations\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=list(relation_counts.keys()), y=list(relation_counts.values()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribution of Relations in the Training Set\")\n",
    "plt.xlabel(\"Relation Types\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 4. Text Tokenization: Average Text Length\n",
    "# --------------------\n",
    "# Analyze token lengths\n",
    "text_lengths = [len(obj['token']) for obj in train_data]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(text_lengths, kde=True, bins=30)\n",
    "plt.title(\"Distribution of Text Lengths (Number of Tokens)\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------\n",
    "# 5. Visualize Sample Images\n",
    "# --------------------\n",
    "sample_img_ids = [obj['img_id'] for obj in train_data[:5]]  # First 5 sample images\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15,5))\n",
    "for i, img_id in enumerate(sample_img_ids):\n",
    "    img_path = os.path.join(IMG_DIRS['train'], img_id)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ****Models apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:06:11.660277Z",
     "iopub.status.busy": "2025-10-14T06:06:11.660029Z",
     "iopub.status.idle": "2025-10-14T06:06:15.462026Z",
     "shell.execute_reply": "2025-10-14T06:06:15.461330Z",
     "shell.execute_reply.started": "2025-10-14T06:06:11.660234Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.42.4 accelerate datasets==2.21.0\n",
    "\n",
    "import os, ast, math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import RobertaModel, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "# -------------- Repro --------------\n",
    "SEED = 1337\n",
    "random.seed(SEED); torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "# -------------- Utilities --------------\n",
    "# def fix_val_path():\n",
    "#     bad = \"/kaggle/working/MNRE/mnre_txt/mnre_val .txt\"\n",
    "#     if os.path.exists(bad) and not os.path.exists(TXT_PATHS['val']):\n",
    "#         os.rename(bad, TXT_PATHS['val'])\n",
    "\n",
    "# fix_val_path()\n",
    "\n",
    "def load_relation2id(train_txt):\n",
    "    rels = set()\n",
    "    with open(train_txt, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj['relation'])\n",
    "    rel2id = {r:i for i,r in enumerate(sorted(rels))}\n",
    "    return rel2id\n",
    "\n",
    "# -------------- Tokenizer --------------\n",
    "TEXT_MODEL = \"roberta-large\"     # 1024-d hidden size\n",
    "tokenizer = RobertaTokenizer.from_pretrained(TEXT_MODEL)\n",
    "\n",
    "# -------------- Dataset --------------\n",
    "class MNREDataset(Dataset):\n",
    "    def __init__(self, txt_file, img_dir, relation2id, max_len=128, train_aug=False):\n",
    "        self.samples = []\n",
    "        self.img_dir = img_dir\n",
    "        self.max_len = max_len\n",
    "        self.relation2id = relation2id\n",
    "\n",
    "        # image transforms\n",
    "        if train_aug:\n",
    "            self.tfm = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomResizedCrop((224,224), scale=(0.8,1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ColorJitter(0.2,0.2,0.2,0.05),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.tfm = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "            ])\n",
    "\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                tokens = obj['token']\n",
    "                h_start, h_end = obj['h']['pos']\n",
    "                t_start, t_end = obj['t']['pos']\n",
    "\n",
    "                # insert entity markers\n",
    "                spans = [('h', h_start, h_end), ('t', t_start, t_end)]\n",
    "                spans.sort(key=lambda x: x[1], reverse=True)\n",
    "                toks = tokens.copy()\n",
    "                for et, s, e in spans:\n",
    "                    toks.insert(e, '[/E1]' if et=='h' else '[/E2]')\n",
    "                    toks.insert(s, '[E1]' if et=='h' else '[E2]')\n",
    "\n",
    "                text = \" \".join(toks)\n",
    "                img_id = obj['img_id']\n",
    "                label = self.relation2id[obj['relation']]\n",
    "                self.samples.append({'text': text, 'img_id': img_id, 'label': label})\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = self.samples[i]\n",
    "        image_path = os.path.join(self.img_dir, s['img_id'])\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img = self.tfm(img)\n",
    "        enc = tokenizer(\n",
    "            s['text'], padding='max_length', truncation=True,\n",
    "            max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'pixel_values': img,\n",
    "            'label': torch.tensor(s['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -------------- Model --------------\n",
    "class CrossModalRE(nn.Module):\n",
    "    \"\"\"\n",
    "    RoBERTa-large (text) + ResNet-101 (image)\n",
    "    - Project image -> 1024\n",
    "    - Cross-attn encoder (2 layers, 16 heads)\n",
    "    - Gating between fused image token & text CLS\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.text_enc = RobertaModel.from_pretrained(TEXT_MODEL)\n",
    "        self.text_hidden = self.text_enc.config.hidden_size  # 1024\n",
    "\n",
    "        # image encoder\n",
    "        cnn = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)\n",
    "        cnn.fc = nn.Identity()\n",
    "        self.img_enc = cnn\n",
    "        self.img_proj = nn.Linear(2048, self.text_hidden)\n",
    "\n",
    "        # cross-attention: put image token at index 0, then text sequence\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.text_hidden, nhead=16,\n",
    "            dim_feedforward=4096, dropout=0.1, activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.xattn = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # gating and classifier\n",
    "        self.gate = nn.Linear(self.text_hidden*2, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.text_hidden, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        # temperature for optional contrastive\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * math.log(1/0.07))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, return_feats=False):\n",
    "        txt = self.text_enc(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        txt_seq = txt.last_hidden_state            # [B, L, 1024]\n",
    "        txt_cls = txt_seq[:,0,:]                   # [B, 1024]\n",
    "\n",
    "        img = self.img_enc(pixel_values)           # [B, 2048]\n",
    "        img = self.img_proj(img).unsqueeze(1)      # [B, 1, 1024]\n",
    "\n",
    "        mm = torch.cat([img, txt_seq], dim=1)      # [B, 1+L, 1024]\n",
    "        fused = self.xattn(mm)                     # [B, 1+L, 1024]\n",
    "        img_token = fused[:,0,:]                   # [B, 1024]\n",
    "\n",
    "        # gating\n",
    "        g = torch.sigmoid(self.gate(torch.cat([txt_cls, img_token], dim=1)))  # [B,1]\n",
    "        fused_vec = g*txt_cls + (1-g)*img_token                                # [B,1024]\n",
    "\n",
    "        logits = self.classifier(fused_vec)\n",
    "        if return_feats:\n",
    "            return logits, fused_vec, img_token\n",
    "        return logits\n",
    "\n",
    "# -------------- Losses --------------\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        n_classes = logits.size(1)\n",
    "        logprobs = self.log_softmax(logits)\n",
    "        nll = -logprobs.gather(dim=1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth = -logprobs.mean(dim=1)\n",
    "        loss = (1 - self.eps) * nll + self.eps * smooth\n",
    "        return loss.mean()\n",
    "\n",
    "def contrastive_loss(z_txt, z_img, temperature):\n",
    "    # z: [B, D] normalized\n",
    "    z_txt = nn.functional.normalize(z_txt, dim=1)\n",
    "    z_img = nn.functional.normalize(z_img, dim=1)\n",
    "    logits = z_txt @ z_img.t() * torch.exp(temperature)\n",
    "    labels = torch.arange(z_txt.size(0), device=z_txt.device)\n",
    "    loss_i = nn.functional.cross_entropy(logits, labels)\n",
    "    loss_t = nn.functional.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# -------------- Train/Eval --------------\n",
    "def compute_class_weights(train_txt, relation2id):\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    with open(train_txt,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            r = ast.literal_eval(line.strip())['relation']\n",
    "            cnt[r]+=1\n",
    "    total = sum(cnt.values())\n",
    "    weights = np.zeros(len(relation2id), dtype=np.float32)\n",
    "    for r, idx in relation2id.items():\n",
    "        # inverse frequency\n",
    "        weights[idx] = total / (len(relation2id)*cnt[r])\n",
    "    # normalize\n",
    "    weights = weights * (len(relation2id)/weights.sum())\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def train_one_epoch(model, loader, optim, sched, ce_loss, alpha_contrast=0.05, scaler=None):\n",
    "    model.train()\n",
    "    losses, preds, labels = [], [], []\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for batch in pbar:\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn = batch['attention_mask'].to(device)\n",
    "        pixels = batch['pixel_values'].to(device)\n",
    "        y = batch['label'].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            logits, fused_vec, img_tok = model(input_ids, attn, pixels, return_feats=True)\n",
    "            loss_cls = ce_loss(logits, y)\n",
    "            # optional contrastive term between fused vec and image token\n",
    "            t = model.logit_scale\n",
    "            loss_ctr = contrastive_loss(fused_vec, img_tok, t) * alpha_contrast\n",
    "            loss = loss_cls + loss_ctr\n",
    "\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optim)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step()\n",
    "\n",
    "        if sched: sched.step()\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(torch.argmax(logits,1).detach().cpu().numpy())\n",
    "        labels.extend(y.detach().cpu().numpy())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return np.mean(losses), acc, f1\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, ce_loss):\n",
    "    model.eval()\n",
    "    losses, preds, labels = [], [], []\n",
    "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn = batch['attention_mask'].to(device)\n",
    "        pixels = batch['pixel_values'].to(device)\n",
    "        y = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids, attn, pixels)\n",
    "        loss = ce_loss(logits, y)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        labels.extend(y.cpu().numpy())\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    print(f\"\\nClassification Report:\\n{report}\")\n",
    "\n",
    "    # Calculate the metrics manually\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    prec = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    return np.mean(losses), acc, f1, prec, rec, report\n",
    "\n",
    "\n",
    "# -------------- Main --------------\n",
    "# Example for the main loop\n",
    "def main():\n",
    "    relation2id = load_relation2id(TXT_PATHS['train'])\n",
    "    num_classes = len(relation2id)\n",
    "    print(\"Relations:\", num_classes, relation2id)\n",
    "\n",
    "    train_ds = MNREDataset(TXT_PATHS['train'], IMG_DIRS['train'], relation2id, train_aug=True)\n",
    "    val_ds   = MNREDataset(TXT_PATHS['val'],   IMG_DIRS['val'],   relation2id, train_aug=False)\n",
    "    test_ds  = MNREDataset(TXT_PATHS['test'],  IMG_DIRS['test'],  relation2id, train_aug=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=12, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = CrossModalRE(num_classes).to(device)\n",
    "\n",
    "    # Loss with label smoothing + class weights\n",
    "    class_w = compute_class_weights(TXT_PATHS['train'], relation2id).to(device)\n",
    "    ce_ls = LabelSmoothingCE(eps=0.1)\n",
    "    # Wrap to apply weights: compute CE with smoothing manually + scale by weights of target\n",
    "    def weighted_loss(logits, target):\n",
    "        base = ce_ls(logits, target)\n",
    "        # lightweight approximation: multiply by per-batch mean weight\n",
    "        w = class_w[target].mean()\n",
    "        return base * w\n",
    "\n",
    "    # Optim & sched\n",
    "    total_steps = len(train_loader) * 10   # 10 epochs default\n",
    "    warmup = int(0.1 * total_steps)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    sched = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup, num_training_steps=total_steps)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    best_f1, best_state = -1, None\n",
    "    patience, bad = 3, 0\n",
    "\n",
    "    for epoch in range(1, 2):\n",
    "        tr_loss, tr_acc, tr_f1 = train_one_epoch(model, train_loader, optim, sched, weighted_loss, alpha_contrast=0.05, scaler=scaler)\n",
    "        vl_loss, vl_acc, vl_f1, vl_p, vl_r, val_report = evaluate(model, val_loader, weighted_loss)\n",
    "        print(f\"Epoch {epoch:02d} | Train: loss {tr_loss:.4f} acc {tr_acc:.3f} f1 {tr_f1:.3f} || Val: loss {vl_loss:.4f} acc {vl_acc:.3f} f1 {vl_f1:.3f}\")\n",
    "\n",
    "        if vl_f1 > best_f1:\n",
    "            best_f1 = vl_f1; bad = 0\n",
    "            best_state = {\n",
    "                'model_state': model.state_dict(),\n",
    "                'relation2id': relation2id,\n",
    "                'config': {'text_model': TEXT_MODEL, 'img_backbone': 'resnet101', 'xattn_layers':2, 'heads':16}\n",
    "            }\n",
    "            torch.save(best_state, 'mnre_roberta_resnet101_xattn_best.pth')\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Test with best checkpoint\n",
    "    if best_state is None:\n",
    "        best_state = torch.load('mnre_roberta_resnet101_xattn_best.pth', map_location=device)\n",
    "        model.load_state_dict(best_state['model_state'])\n",
    "    else:\n",
    "        model.load_state_dict(best_state['model_state'])\n",
    "\n",
    "    te_loss, te_acc, te_f1, te_p, te_r, test_report = evaluate(model, test_loader, weighted_loss)\n",
    "    print(f\"\\nTEST  | acc {te_acc:.4f} f1 {te_f1:.4f}  P {te_p:.4f} R {te_r:.4f}\")\n",
    "    print(f\"\\nTest Classification Report:\\n{test_report}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:06:15.468537Z",
     "iopub.status.busy": "2025-10-14T06:06:15.468314Z",
     "iopub.status.idle": "2025-10-14T06:06:16.927336Z",
     "shell.execute_reply": "2025-10-14T06:06:16.926520Z",
     "shell.execute_reply.started": "2025-10-14T06:06:15.468512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data from the classification report\n",
    "classification_report = {\n",
    "    '0': {'precision': 0.9368421052631579, 'recall': 0.898989898989899, 'f1-score': 0.9175257731958762, 'support': 99},\n",
    "    '1': {'precision': 1.0, 'recall': 0.16666666666666666, 'f1-score': 0.2857142857142857, 'support': 18},\n",
    "    '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 14},\n",
    "    '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 46},\n",
    "    '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 8},\n",
    "    '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 16},\n",
    "    '6': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 7},\n",
    "    '7': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 29},\n",
    "    '8': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4},\n",
    "    '9': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 10},\n",
    "    '10': {'precision': 0.3333333333333333, 'recall': 0.013513513513513514, 'f1-score': 0.025974025974025972, 'support': 74},\n",
    "    '12': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
    "    '13': {'precision': 0.43356643356643354, 'recall': 0.5636363636363636, 'f1-score': 0.49011857707509876, 'support': 110},\n",
    "    '14': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 21},\n",
    "    '16': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
    "    '17': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 19},\n",
    "    '18': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2},\n",
    "    '19': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 4},\n",
    "    '20': {'precision': 0.5492227979274611, 'recall': 0.6794871794871795, 'f1-score': 0.6074498567335244, 'support': 156},\n",
    "    '21': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
    "    '22': {'precision': 0.7612574341546304, 'recall': 0.919917864476386, 'f1-score': 0.8331008833100884, 'support': 974},\n",
    "    'accuracy': 0.7168525402726146,\n",
    "    'macro avg': {'precision': 0.19115343353547698, 'recall': 0.1543910231795242, 'f1-score': 0.15047063819061424, 'support': 1614},\n",
    "    'weighted avg': {'precision': 0.6259292689735569, 'recall': 0.7168525402726146, 'f1-score': 0.6555238340049836, 'support': 1614}\n",
    "}\n",
    "\n",
    "# Handling accuracy, macro avg, and weighted avg separately\n",
    "# Add these values to the dictionary\n",
    "classification_report['accuracy'] = {\n",
    "    'precision': classification_report['accuracy'],\n",
    "    'recall': classification_report['accuracy'],\n",
    "    'f1-score': classification_report['accuracy'],\n",
    "    'support': sum([entry['support'] for entry in classification_report.values() if isinstance(entry, dict)])\n",
    "}\n",
    "\n",
    "classification_report['macro avg'] = classification_report['macro avg']\n",
    "classification_report['weighted avg'] = classification_report['weighted avg']\n",
    "\n",
    "# Remove the 'accuracy', 'macro avg', and 'weighted avg' from individual class scores\n",
    "del classification_report['accuracy']\n",
    "del classification_report['macro avg']\n",
    "del classification_report['weighted avg']\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "report_df = pd.DataFrame.from_dict(classification_report, orient='index')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:06:16.930244Z",
     "iopub.status.busy": "2025-10-14T06:06:16.929993Z",
     "iopub.status.idle": "2025-10-14T06:06:17.773869Z",
     "shell.execute_reply": "2025-10-14T06:06:17.773031Z",
     "shell.execute_reply.started": "2025-10-14T06:06:16.930226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:06:17.774792Z",
     "iopub.status.busy": "2025-10-14T06:06:17.774592Z",
     "iopub.status.idle": "2025-10-14T06:06:18.613737Z",
     "shell.execute_reply": "2025-10-14T06:06:18.612934Z",
     "shell.execute_reply.started": "2025-10-14T06:06:17.774773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming `relation2id` is already loaded from your dataset\n",
    "relation2id = load_relation2id(TXT_PATHS['train'])\n",
    "\n",
    "# Print all relations\n",
    "print(\"Relations in the dataset:\")\n",
    "for relation, idx in relation2id.items():\n",
    "    print(f\"{idx}: {relation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T06:08:22.307495Z",
     "iopub.status.busy": "2025-10-14T06:08:22.306894Z",
     "iopub.status.idle": "2025-10-14T07:01:36.829847Z",
     "shell.execute_reply": "2025-10-14T07:01:36.829175Z",
     "shell.execute_reply.started": "2025-10-14T06:08:22.307473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "# Initialize CLIP and BERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize CLIP model and processor from transformers\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load BERT tokenizer and model (for text features)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# ========================\n",
    "# 1. Dataset\n",
    "# ========================\n",
    "class MNREDataset(Dataset):\n",
    "    def __init__(self, txt_file, img_dir, relation2id, max_len=128, transform=None):\n",
    "        self.samples = []\n",
    "        self.img_dir = img_dir\n",
    "        self.max_len = max_len\n",
    "        self.relation2id = relation2id\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                tokens = obj['token']\n",
    "                h_start, h_end = obj['h']['pos']\n",
    "                t_start, t_end = obj['t']['pos']\n",
    "\n",
    "                # Insert entity markers\n",
    "                spans = sorted([('h', h_start, h_end), ('t', t_start, t_end)], key=lambda x: x[1], reverse=True)\n",
    "                for etype, start, end in spans:\n",
    "                    tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "                    tokens.insert(end, tag_close)\n",
    "                    tag_open = '[E1]' if etype == 'h' else '[E2]'\n",
    "                    tokens.insert(start, tag_open)\n",
    "\n",
    "                text = \" \".join(tokens)\n",
    "                img_id = obj['img_id']\n",
    "                label = relation2id[obj['relation']]\n",
    "                self.samples.append({\"text\": text, \"img_id\": img_id, \"label\": label})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_path = os.path.join(self.img_dir, sample['img_id'])\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img = self.transform(img) if self.transform else img\n",
    "        text = sample['text']\n",
    "\n",
    "        # Tokenize the text\n",
    "        text_inputs = bert_tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        # Preprocess image for CLIP\n",
    "        image_inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_inputs[\"input_ids\"].squeeze(0).to(device),\n",
    "            \"attention_mask\": text_inputs[\"attention_mask\"].squeeze(0).to(device),\n",
    "            \"pixel_values\": image_inputs.pixel_values.squeeze(0),\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long).to(device)\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# 2. Model (Relation Classifier)\n",
    "# ========================\n",
    "class CrossModalRE(nn.Module):\n",
    "    def __init__(self, num_relations):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CLIP image encoder (using CLIP model)\n",
    "        self.clip_model = clip_model\n",
    "        \n",
    "        # BERT text encoder (using BERT model)\n",
    "        self.bert_model = bert_model\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        # The input size here is 512 for CLIP + 768 for BERT = 1280\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1280, 512),  # Adjusted to match the combined embedding size (512 + 768)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_relations)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Get text embeddings from BERT\n",
    "        text_outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # Get [CLS] token representation\n",
    "        \n",
    "        # Get image embeddings from CLIP\n",
    "        image_outputs = self.clip_model.get_image_features(pixel_values=pixel_values)\n",
    "        \n",
    "        # Concatenate text and image embeddings\n",
    "        combined_embeddings = torch.cat((text_embeddings, image_outputs), dim=1)  # Shape should be [batch_size, 1280]\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classifier(combined_embeddings)  # Output size [batch_size, num_relations]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 3. Training & Evaluation\n",
    "# ========================\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    losses, preds, labels = [], [], []\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch[\"attention_mask\"].to(device)\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        labs = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attn, pixels)\n",
    "        loss = criterion(logits, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return sum(losses)/len(losses), accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, preds, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            pixels = batch[\"pixel_values\"].to(device)\n",
    "            labs = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attn, pixels)\n",
    "            loss = criterion(logits, labs)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "            labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return sum(losses)/len(losses), accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "# ========================\n",
    "# 4. Main\n",
    "# ========================\n",
    "def main():\n",
    "    txt_paths = {\n",
    "        'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "        'val': '/kaggle/working/MNRE/mnre_txt/mnre_val .txt',\n",
    "        'test': '/kaggle/working/MNRE/mnre_txt/mnre_test.txt'\n",
    "    }\n",
    "\n",
    "    img_dirs = {\n",
    "        'train': '/kaggle/working/img_org/train',\n",
    "        'val': '/kaggle/working/img_org/val',\n",
    "        'test': '/kaggle/working/img_org/test'\n",
    "    }\n",
    "\n",
    "    # Load relation2id mapping\n",
    "    rels = set()\n",
    "    with open(txt_paths[\"train\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj[\"relation\"])\n",
    "    relation2id = {r: idx for idx, r in enumerate(sorted(rels))}\n",
    "    num_relations = len(relation2id)\n",
    "\n",
    "    # Prepare datasets & dataloaders\n",
    "    datasets = {\n",
    "        split: MNREDataset(txt_paths[split], img_dirs[split], relation2id)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    loaders = {\n",
    "        split: DataLoader(datasets[split], batch_size=8, shuffle=(split == \"train\"))\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "\n",
    "    # Instantiate model, criterion, and optimizer\n",
    "    model = CrossModalRE(num_relations).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    EPOCHS = 2\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, tr_acc, tr_f1 = train_epoch(model, loaders[\"train\"], criterion, optimizer)\n",
    "        val_loss, val_acc, val_f1 = eval_epoch(model, loaders[\"val\"], criterion)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | Train Acc: {tr_acc:.4f} F1: {tr_f1:.4f} | Val Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Test Evaluation\n",
    "    test_loss, test_acc, test_f1 = eval_epoch(model, loaders[\"test\"], criterion)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T07:02:57.958803Z",
     "iopub.status.busy": "2025-10-14T07:02:57.958242Z",
     "iopub.status.idle": "2025-10-14T07:02:59.378639Z",
     "shell.execute_reply": "2025-10-14T07:02:59.377819Z",
     "shell.execute_reply.started": "2025-10-14T07:02:57.958782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "best_state = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'relation2id': relation2id,\n",
    "    'config': {\n",
    "        'text_model': 'bert-base-uncased',\n",
    "        'image_model': 'openai/clip-vit-base-patch16'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the state dictionary to a .pth file\n",
    "torch.save(best_state, 'cross_modal_re_clip_bert_finetuned.pth')\n",
    "\n",
    "print(\"Model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T07:05:23.767952Z",
     "iopub.status.busy": "2025-10-14T07:05:23.767321Z",
     "iopub.status.idle": "2025-10-14T07:05:25.422400Z",
     "shell.execute_reply": "2025-10-14T07:05:25.421660Z",
     "shell.execute_reply.started": "2025-10-14T07:05:23.767921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your saved model\n",
    "checkpoint = torch.load('/kaggle/working/cross_modal_re_clip_bert_finetuned.pth')\n",
    "model = CrossModalRE(len(checkpoint['relation2id'])).to(device)  # Assuming num_relations = len of relation2id\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "# Load tokenizer and processor\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# Load the relation2id mapping from your checkpoint\n",
    "relation2id = checkpoint['relation2id']\n",
    "id2relation = {v: k for k, v in relation2id.items()}  # Create a reverse mapping to get relation text from id\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to perform prediction and return relation text\n",
    "def predict_relation(image_path, text, model, bert_tokenizer, clip_processor, id2relation):\n",
    "    # Open image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Preprocess the image for CLIP\n",
    "    image_inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Tokenize the text using BERT tokenizer\n",
    "    text_inputs = bert_tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Make the prediction\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=text_inputs['input_ids'], attention_mask=text_inputs['attention_mask'], pixel_values=image_inputs['pixel_values'])\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()  # Get the predicted class index\n",
    "        \n",
    "        # Get the predicted relation text\n",
    "        predicted_relation_text = id2relation[predicted_class]\n",
    "        \n",
    "    return predicted_class, predicted_relation_text, image\n",
    "\n",
    "# Example usage for a given image and text\n",
    "image_path = '/kaggle/working/img_org/test/twitter_19_31_0_13.jpg'  # Path to your image\n",
    "text = \"Some example text that describes the relation\"  # Example relation description\n",
    "\n",
    "# Get the predicted relation and the image\n",
    "predicted_class, predicted_relation, image = predict_relation(image_path, text, model, bert_tokenizer, clip_processor, id2relation)\n",
    "\n",
    "# Plot the image with the predicted relation\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Predicted Relation: {predicted_relation} (ID: {predicted_class})\")\n",
    "plt.show()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Image Path: {image_path}\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted Relation (ID): {predicted_class}\")\n",
    "print(f\"Predicted Relation (Text): {predicted_relation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and NER apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.976643Z",
     "iopub.status.idle": "2025-10-14T06:06:22.976888Z",
     "shell.execute_reply": "2025-10-14T06:06:22.976787Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.976777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import pipeline, RobertaTokenizer, AutoModelForTokenClassification, BertTokenizer\n",
    "\n",
    "# Assuming you have a trained model saved as 'mnre_roberta_resnet101_xattn_best.pth'\n",
    "checkpoint = torch.load('/kaggle/working/mnre_roberta_resnet101_xattn_best.pth', map_location=device)\n",
    "model = CrossModalRE(num_classes=len(checkpoint['relation2id'])).to(device)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.eval()\n",
    "\n",
    "# Load the pre-trained NER model and tokenizer\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "ner_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "ner_pipe = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=0)\n",
    "\n",
    "# --------------------------\n",
    "# Sample images and predictions\n",
    "# --------------------------\n",
    "\n",
    "# Assuming `train_data` is already defined and holds the dataset\n",
    "sample_img_ids = [obj['img_id'] for obj in train_data[:5]]  # First 5 sample images\n",
    "\n",
    "# Load images and show them\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15,5))\n",
    "for i, img_id in enumerate(sample_img_ids):\n",
    "    img_path = os.path.join(IMG_DIRS['train'], img_id)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define the function to predict relation for each image and extract text\n",
    "def predict_relation_for_image(image_path, tokens, model, tokenizer, relation2id, max_len=128):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Tokenizing the input text (from the dataset)\n",
    "    text = \" \".join(tokens)  # Combine tokens to make the input text\n",
    "    enc = tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt').to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(enc['input_ids'], enc['attention_mask'], img_tensor)\n",
    "\n",
    "    # Process the logits for relation prediction\n",
    "    relation_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Map relation ID back to relation name\n",
    "    id2relation = {v: k for k, v in relation2id.items()}  # Reverse the relation2id dictionary\n",
    "    relation_name = id2relation.get(relation_preds[0], \"Unknown\")  # Get relation name\n",
    "\n",
    "    return relation_name, text  # Returning the relation prediction along with the text\n",
    "\n",
    "# Now predict relations for the first 5 images and get the text associated with each image\n",
    "predictions = []\n",
    "for img_id in sample_img_ids:\n",
    "    img_path = os.path.join(IMG_DIRS['train'], img_id)\n",
    "    \n",
    "    # Extract the tokens (text) associated with the image from your dataset\n",
    "    sample = next(item for item in train_data if item['img_id'] == img_id)\n",
    "    tokens = sample['token']  # Extract tokens associated with this image\n",
    "    \n",
    "    # Predict relation and extract text\n",
    "    relation, extracted_text = predict_relation_for_image(img_path, tokens, model, tokenizer, checkpoint['relation2id'])\n",
    "    \n",
    "    # Apply NER to the extracted text\n",
    "    ner_results = ner_pipe(extracted_text)\n",
    "    \n",
    "    predictions.append((img_id, relation, extracted_text, ner_results))\n",
    "\n",
    "# Display predicted relations, extracted text, and NER results for the images\n",
    "for i, (img_id, rel, extracted_text, ner_results) in enumerate(predictions):\n",
    "    print(f\"Image {i+1} ({img_id}) Predicted Relation: {rel}\")\n",
    "    print(f\"Extracted Text: {extracted_text}\")\n",
    "    print(\"NER Results:\")\n",
    "    for ner in ner_results:\n",
    "        print(f\"Entity: {ner['word']} | Type: {ner['entity']} | Score: {ner['score']:.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet34 + bert-base-uncased models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.978041Z",
     "iopub.status.idle": "2025-10-14T06:06:22.978295Z",
     "shell.execute_reply": "2025-10-14T06:06:22.978196Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.978184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.979203Z",
     "iopub.status.idle": "2025-10-14T06:06:22.979440Z",
     "shell.execute_reply": "2025-10-14T06:06:22.979331Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.979319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MNREDataset(Dataset):\n",
    "    def __init__(self, txt_file, img_dir, tokenizer, relation2id, max_len=128, transform=None):\n",
    "        self.txt_file = txt_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.relation2id = relation2id\n",
    "        self.samples = []\n",
    "        with open(self.txt_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                tokens = obj['token']\n",
    "                h_start, h_end = obj['h']['pos']\n",
    "                t_start, t_end = obj['t']['pos']\n",
    "                spans = sorted([('h', h_start, h_end), ('t', t_start, t_end)], key=lambda x: x[1], reverse=True)\n",
    "                for etype, start, end in spans:\n",
    "                    tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "                    tokens.insert(end, tag_close)\n",
    "                    tag_open = '[E1]' if etype == 'h' else '[E2]'\n",
    "                    tokens.insert(start, tag_open)\n",
    "                text = \" \".join(tokens)\n",
    "                img_id = obj['img_id']\n",
    "                label = self.relation2id[obj['relation']]\n",
    "                self.samples.append({'text': text, 'img_id': img_id, 'label': label})\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        enc = self.tokenizer(sample['text'], padding='max_length', truncation=True,\n",
    "                             max_length=self.max_len, return_tensors='pt')\n",
    "        img_path = os.path.join(self.img_dir, sample['img_id'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform: image = self.transform(image)\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'image': image,\n",
    "            'label': torch.tensor(sample['label'], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.980319Z",
     "iopub.status.idle": "2025-10-14T06:06:22.980534Z",
     "shell.execute_reply": "2025-10-14T06:06:22.980440Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.980431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =====================\n",
    "# Model Architecture\n",
    "# =====================\n",
    "class MultimodalREModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # ---- Text encoder (BERT) ----\n",
    "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # ---- Image encoder (ResNet-34) ----\n",
    "        cnn = models.resnet34(pretrained=True)\n",
    "        cnn.fc = nn.Identity()  # remove classification head\n",
    "        self.image_encoder = cnn\n",
    "\n",
    "        # ---- Projection layer to match text hidden size ----\n",
    "        self.img_proj = nn.Linear(512, self.text_encoder.config.hidden_size)\n",
    "\n",
    "        # ---- Cross-Attention (Transformer encoder layer) ----\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.text_encoder.config.hidden_size,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            activation=\"relu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.cross_attention = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        # ---- Classifier ----\n",
    "        hidden_dim = self.text_encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Encode text\n",
    "        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        txt_feats = txt_out.last_hidden_state   # [B, seq_len, H]\n",
    "\n",
    "        # Encode image\n",
    "        img_feats = self.image_encoder(image)   # [B, 512]\n",
    "        img_feats = self.img_proj(img_feats).unsqueeze(1)  # [B, 1, H]\n",
    "\n",
    "        # Concatenate image embedding as a token to text sequence\n",
    "        multimodal_feats = torch.cat([img_feats, txt_feats], dim=1)  # [B, 1+seq_len, H]\n",
    "\n",
    "        # Cross-attention fusion\n",
    "        fused_feats = self.cross_attention(multimodal_feats)  # [B, 1+seq_len, H]\n",
    "\n",
    "        # Use the image token ([IMG]) as global fused representation\n",
    "        img_token = fused_feats[:, 0, :]  # [B, H]\n",
    "\n",
    "        return self.classifier(img_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.981545Z",
     "iopub.status.idle": "2025-10-14T06:06:22.981865Z",
     "shell.execute_reply": "2025-10-14T06:06:22.981715Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.981701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses, preds, labels = [], [], []\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labs = batch['label'].to(device)\n",
    "        outs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outs, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        pred = torch.argmax(outs, dim=1).cpu().numpy()\n",
    "        preds.extend(pred)\n",
    "        labels.extend(labs.cpu().numpy())\n",
    "    return {\n",
    "        'loss': sum(losses)/len(losses),\n",
    "        'acc': accuracy_score(labels, preds),\n",
    "        'prec': precision_score(labels, preds, average='macro'),\n",
    "        'rec': recall_score(labels, preds, average='macro'),\n",
    "        'f1': f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses, preds, labels, probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labs = batch['label'].to(device)\n",
    "            outs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outs, labs)\n",
    "            prob = torch.softmax(outs, dim=1)\n",
    "            losses.append(loss.item())\n",
    "            pred = torch.argmax(outs, dim=1).cpu().numpy()\n",
    "            preds.extend(pred)\n",
    "            labels.extend(labs.cpu().numpy())\n",
    "            if prob.shape[1] == 2:\n",
    "                probs.extend(prob[:,1].cpu().numpy())\n",
    "    metrics = {\n",
    "        'loss': sum(losses)/len(losses),\n",
    "        'acc': accuracy_score(labels, preds),\n",
    "        'prec': precision_score(labels, preds, average='macro'),\n",
    "        'rec': recall_score(labels, preds, average='macro'),\n",
    "        'f1': f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "    roc_data = None\n",
    "    if len(set(labels)) == 2:\n",
    "        fpr, tpr, _ = roc_curve(labels, probs)\n",
    "        metrics['roc_auc'] = auc(fpr, tpr)\n",
    "        roc_data = (fpr, tpr)\n",
    "    return metrics, roc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert-base-uncased models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.982749Z",
     "iopub.status.idle": "2025-10-14T06:06:22.982972Z",
     "shell.execute_reply": "2025-10-14T06:06:22.982879Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.982865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    epochs, batch_size, lr = 5, 16, 1e-5\n",
    "    txt_paths = {\n",
    "        'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "        'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val .txt',  # Fixed typo in path\n",
    "        'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt'\n",
    "    }\n",
    "    img_dirs = {\n",
    "        'train': '/kaggle/working/img_org/train',\n",
    "        'val':   '/kaggle/working/img_org/val',\n",
    "        'test':  '/kaggle/working/img_org/test'\n",
    "    }\n",
    "    # Build relation2id from training data\n",
    "    rels = set()\n",
    "    with open(txt_paths['train'], 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj['relation'])\n",
    "    relation2id = {r: idx for idx, r in enumerate(sorted(rels))}\n",
    "    num_classes = len(relation2id)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "\n",
    "    datasets = {split: MNREDataset(txt_paths[split], img_dirs[split], tokenizer, relation2id, transform=transform)\n",
    "                for split in ('train', 'val', 'test')}\n",
    "    loaders = {split: DataLoader(datasets[split], batch_size=batch_size, shuffle=(split == 'train'))\n",
    "               for split in ('train', 'val', 'test')}\n",
    "\n",
    "    model = MultimodalREModel(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    history = {'train': [], 'val': []}\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_metrics = train_epoch(model, loaders['train'], criterion, optimizer, device)\n",
    "        val_metrics, _ = eval_epoch(model, loaders['val'], criterion, device)\n",
    "        history['train'].append(train_metrics)\n",
    "        history['val'].append(val_metrics)\n",
    "        print(f\"Epoch {epoch}/{epochs} | Training Accuracy: {train_metrics['acc']:.4f} | Val Accuracy: {val_metrics['acc']:.4f} | \"\n",
    "              f\"Train F1: {train_metrics['f1']:.4f} | Val F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save({'model_state': model.state_dict(), 'relation2id': relation2id}, 'mnre_model.pth')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_metrics, roc_data = eval_epoch(model, loaders['test'], criterion, device)\n",
    "    print(f\"Test Metrics: Training Accuracy={test_metrics['acc']:.4f}, F1={test_metrics['f1']:.4f}, \"\n",
    "          f\"Precision={test_metrics['prec']:.4f}, Recall={test_metrics['rec']:.4f}, \"\n",
    "          f\"ROC AUC={test_metrics.get('roc_auc', 'N/A')}\")\n",
    "\n",
    "    # Plot Training Accuracy and F1 Score\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_range, [m['acc'] for m in history['train']], label='Training Accuracy')\n",
    "    plt.plot(epochs_range, [m['acc'] for m in history['val']], label='Val Accuracy')\n",
    "    plt.plot(epochs_range, [m['f1'] for m in history['train']], '--', label='Train F1')\n",
    "    plt.plot(epochs_range, [m['f1'] for m in history['val']], '--', label='Val F1')\n",
    "    plt.title('Training Accuracy & F1 Score over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Precision and Recall for Training\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_range, [m['prec'] for m in history['train']], label='Train Precision')\n",
    "    plt.plot(epochs_range, [m['rec'] for m in history['train']], label='Train Recall')\n",
    "    plt.title('Training Precision & Recall over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if roc_data:\n",
    "        fpr, tpr = roc_data\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(fpr, tpr, label=f\"ROC (AUC={test_metrics['roc_auc']:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "        plt.title('ROC Curve (Test Set)')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return tokenizer, transform, device, relation2id  # Return for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.983949Z",
     "iopub.status.idle": "2025-10-14T06:06:22.984243Z",
     "shell.execute_reply": "2025-10-14T06:06:22.984083Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.984069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer, transform, device, relation2id = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.985271Z",
     "iopub.status.idle": "2025-10-14T06:06:22.985474Z",
     "shell.execute_reply": "2025-10-14T06:06:22.985384Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.985375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('mnre_model.pth')\n",
    "relation2id = checkpoint['relation2id']\n",
    "id2relation = {v: k for k, v in relation2id.items()}\n",
    "model = MultimodalREModel(num_classes=len(relation2id))\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.986207Z",
     "iopub.status.idle": "2025-10-14T06:06:22.986494Z",
     "shell.execute_reply": "2025-10-14T06:06:22.986337Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.986325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import os\n",
    "\n",
    "\n",
    "# Relation Predictor class\n",
    "class RelationPredictor:\n",
    "    def __init__(self, model_path, dataset_path='/kaggle/working/MNRE/mnre_txt/mnre_train.txt', image_dir='/kaggle/working/img_org'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # Load the trained model\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.relation2id = checkpoint['relation2id']\n",
    "        self.id2relation = {v: k for k, v in self.relation2id.items()}\n",
    "        self.model = MultimodalREModel(num_classes=len(self.relation2id))\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        # Load tokenizer and image transform\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        # Load dataset\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            self.dataset = [ast.literal_eval(line.strip()) for line in f]\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def find_sample(self, tokens):\n",
    "        for sample in self.dataset:\n",
    "            if sample['token'] == tokens:\n",
    "                return sample\n",
    "        raise ValueError(\"No matching sample found for the given token list\")\n",
    "\n",
    "    def preprocess_text(self, tokens, h_pos, t_pos):\n",
    "        tokens = tokens.copy()\n",
    "        spans = [('h', h_pos[0], h_pos[1]), ('t', t_pos[0], t_pos[1])]\n",
    "        spans.sort(key=lambda x: x[1], reverse=True)\n",
    "        for etype, start, end in spans:\n",
    "            tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "            tokens.insert(end, tag_close)\n",
    "            tag_open = '[E1]' if etype == 'h' else '[E2]'\n",
    "            tokens.insert(start, tag_open)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def predict_from_token(self, tokens):\n",
    "        sample = self.find_sample(tokens)\n",
    "        h_pos = sample['h']['pos']\n",
    "        t_pos = sample['t']['pos']\n",
    "        h_name = sample['h']['name']\n",
    "        t_name = sample['t']['name']\n",
    "        img_id = sample['img_id']\n",
    "        image_path = os.path.join(self.image_dir, img_id)\n",
    "\n",
    "        processed_text = self.preprocess_text(tokens, h_pos, t_pos)\n",
    "        enc = self.tokenizer(processed_text, padding='max_length', truncation=True,\n",
    "                            max_length=128, return_tensors='pt')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        input_ids = enc['input_ids'].to(self.device)\n",
    "        attention_mask = enc['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask, image_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            pred_idx = torch.argmax(probs, dim=1).item()\n",
    "            pred_relation = self.id2relation[pred_idx]\n",
    "            confidence = probs[0, pred_idx].item()\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(f\"Processed Text: {processed_text}\")\n",
    "        print(f\"Head Entity: {h_name}\")\n",
    "        print(f\"Tail Entity: {t_name}\")\n",
    "        print(f\"Predicted Relation: {pred_relation} with confidence {confidence:.4f}\")\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'processed_text': processed_text,\n",
    "            'head_entity': h_name,\n",
    "            'tail_entity': t_name,\n",
    "            'predicted_relation': pred_relation,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "predictor = RelationPredictor(\n",
    "    model_path='/kaggle/working/mnre_model.pth',\n",
    "    dataset_path='/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "    image_dir='/kaggle/working/img_org/train'\n",
    ")\n",
    "tokens = ['The', 'latest', 'Arkham', 'Horror', 'LCG', 'deluxe', 'expansion',\n",
    "          'the', 'Circle', 'Undone', 'has', 'been', 'released', ':']\n",
    "result = predictor.predict_from_token(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.987872Z",
     "iopub.status.idle": "2025-10-14T06:06:22.988174Z",
     "shell.execute_reply": "2025-10-14T06:06:22.988002Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.987987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "predictor = RelationPredictor(\n",
    "    model_path='/kaggle/working/mnre_model.pth',\n",
    "    dataset_path='/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "    image_dir='/kaggle/working/img_org/train'\n",
    ")\n",
    "tokens = ['RT', '@PaulStrangwood', ':', '@ThePhotoHour', 'this', 'is', 'part', 'of', 'the', 'Magat', 'Damms', 'near', 'to', 'Cauayan', 'in', 'the', 'Philippines']\n",
    "result = predictor.predict_from_token(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.989343Z",
     "iopub.status.idle": "2025-10-14T06:06:22.989581Z",
     "shell.execute_reply": "2025-10-14T06:06:22.989492Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.989483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import os\n",
    "\n",
    "class RelationPredictor:\n",
    "    def __init__(self, model_path, dataset_path, image_dir):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.relation2id = checkpoint['relation2id']\n",
    "        self.id2relation = {v: k for k, v in self.relation2id.items()}\n",
    "        self.model = MultimodalREModel(num_classes=len(self.relation2id))\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            self.dataset = [ast.literal_eval(line.strip()) for line in f]\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def find_sample(self, tokens):\n",
    "        for sample in self.dataset:\n",
    "            if sample['token'] == tokens:\n",
    "                return sample\n",
    "        raise ValueError(\"‚ùå No matching sample found for the given tokens.\")\n",
    "\n",
    "    def preprocess_text(self, tokens, h_pos, t_pos):\n",
    "        tokens = tokens.copy()\n",
    "        spans = [('h', h_pos[0], h_pos[1]), ('t', t_pos[0], t_pos[1])]\n",
    "        spans.sort(key=lambda x: x[1], reverse=True)\n",
    "        for etype, start, end in spans:\n",
    "            tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "            tokens.insert(end, tag_close)\n",
    "            tag_open = '[E1]' if etype == 'h' else '[E2]'\n",
    "            tokens.insert(start, tag_open)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def predict_from_token(self, tokens):\n",
    "        sample = self.find_sample(tokens)\n",
    "        h_pos = sample['h']['pos']\n",
    "        t_pos = sample['t']['pos']\n",
    "        h_name = sample['h']['name']\n",
    "        t_name = sample['t']['name']\n",
    "        img_id = sample['img_id']\n",
    "        image_path = os.path.join(self.image_dir, img_id)\n",
    "\n",
    "        processed_text = self.preprocess_text(tokens, h_pos, t_pos)\n",
    "        enc = self.tokenizer(processed_text, padding='max_length', truncation=True,\n",
    "                             max_length=128, return_tensors='pt')\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        input_ids = enc['input_ids'].to(self.device)\n",
    "        attention_mask = enc['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, attention_mask, image_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            pred_idx = torch.argmax(probs, dim=1).item()\n",
    "            pred_relation = self.id2relation[pred_idx]\n",
    "            confidence = probs[0, pred_idx].item()\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"üìù Processed Text: {processed_text}\")\n",
    "        print(f\"üë§ Head Entity: {h_name}\")\n",
    "        print(f\"üë• Tail Entity: {t_name}\")\n",
    "        print(f\"üîó Predicted Relation: {pred_relation}\")\n",
    "        print(f\"üìä Confidence Score: {confidence:.4f}\")\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'processed_text': processed_text,\n",
    "            'head_entity': h_name,\n",
    "            'tail_entity': t_name,\n",
    "            'predicted_relation': pred_relation,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "# === Usage Example ===\n",
    "\n",
    "tokens = ['RT', '@TedNesi', ':', 'Bishop', 'Tobin', 'says', 'he', 'is', 'leaving', 'Twitter', 'after', 'just', 'a', 'few', 'months']\n",
    "\n",
    "predictor = RelationPredictor(\n",
    "    model_path='/kaggle/working/mnre_model.pth',\n",
    "    dataset_path='/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "    image_dir='/kaggle/working/img_org/train'\n",
    ")\n",
    "\n",
    "predictor.predict_from_token(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained model BLip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.990309Z",
     "iopub.status.idle": "2025-10-14T06:06:22.990604Z",
     "shell.execute_reply": "2025-10-14T06:06:22.990466Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.990453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from transformers import BlipProcessor, BlipModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# 1. Load BLIP (pretrained vision-language model)\n",
    "# ========================\n",
    "MODEL_NAME = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME)\n",
    "blip_model = BlipModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# Freeze BLIP encoder (optional)\n",
    "for param in blip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "HIDDEN_SIZE = blip_model.config.text_config.hidden_size  # 768\n",
    "\n",
    "# ========================\n",
    "# 2. Dataset\n",
    "# ========================\n",
    "class MNREDataset(Dataset):\n",
    "    def __init__(self, txt_file, img_dir, relation2id, transform=None, max_len=128):\n",
    "        self.samples = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                tokens = obj['token']\n",
    "                h_start, h_end = obj['h']['pos']\n",
    "                t_start, t_end = obj['t']['pos']\n",
    "\n",
    "                # Insert entity markers\n",
    "                spans = sorted([('h', h_start, h_end), ('t', t_start, t_end)], key=lambda x: x[1], reverse=True)\n",
    "                for etype, start, end in spans:\n",
    "                    tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "                    tokens.insert(end, tag_close)\n",
    "                    tag_open = '[E1]' if etype == 'h' else '[E2]'\n",
    "                    tokens.insert(start, tag_open)\n",
    "\n",
    "                text = \" \".join(tokens)\n",
    "                img_id = obj['img_id']\n",
    "                label = relation2id[obj['relation']]\n",
    "\n",
    "                self.samples.append({\"text\": text, \"img_id\": img_id, \"label\": label})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image_path = os.path.join(self.img_dir, sample['img_id'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = processor(images=image, text=sample['text'], padding=\"max_length\", truncation=True,\n",
    "                           max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# 3. Model (Relation Classifier)\n",
    "# ========================\n",
    "class MultimodalREModel(nn.Module):\n",
    "    def __init__(self, num_relations):\n",
    "        super().__init__()\n",
    "        self.blip = blip_model\n",
    "\n",
    "        # üîπ Get actual BLIP hidden size (can be 512 or 768 depending on model variant)\n",
    "        hidden_size = self.blip.config.projection_dim  \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_relations)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        outputs = self.blip(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            pixel_values=pixel_values)\n",
    "\n",
    "        pooled = outputs.image_embeds  # or outputs.text_embeds (both are projection_dim size)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 4. Training & Evaluation\n",
    "# ========================\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    losses, preds, labels = [], [], []\n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch[\"attention_mask\"].to(device)\n",
    "        pixels = batch[\"pixel_values\"].to(device)\n",
    "        labs = batch[\"label\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attn, pixels)\n",
    "        loss = criterion(logits, labs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "        labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return sum(losses)/len(losses), accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses, preds, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attn = batch[\"attention_mask\"].to(device)\n",
    "            pixels = batch[\"pixel_values\"].to(device)\n",
    "            labs = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attn, pixels)\n",
    "            loss = criterion(logits, labs)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.extend(torch.argmax(logits, 1).cpu().numpy())\n",
    "            labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    return sum(losses)/len(losses), accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "# ========================\n",
    "# 5. Main\n",
    "# ========================\n",
    "def main():\n",
    "    txt_paths = {\n",
    "        'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "        'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val .txt',\n",
    "        'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt'\n",
    "    }\n",
    "    img_dirs = {\n",
    "        'train': '/kaggle/working/img_org/train',\n",
    "        'val':   '/kaggle/working/img_org/val',\n",
    "        'test':  '/kaggle/working/img_org/test'\n",
    "    }\n",
    "\n",
    "    # Build relation mapping\n",
    "    rels = set()\n",
    "    with open(txt_paths[\"train\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj[\"relation\"])\n",
    "    relation2id = {r: idx for idx, r in enumerate(sorted(rels))}\n",
    "    num_relations = len(relation2id)\n",
    "\n",
    "    # Datasets & Loaders\n",
    "    datasets = {\n",
    "        split: MNREDataset(txt_paths[split], img_dirs[split], relation2id)\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "    loaders = {\n",
    "        split: DataLoader(datasets[split], batch_size=8, shuffle=(split==\"train\"))\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "\n",
    "    # Model, Loss, Optimizer\n",
    "    model = MultimodalREModel(num_relations).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    EPOCHS = 5\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc, tr_f1 = train_epoch(model, loaders[\"train\"], criterion, optimizer)\n",
    "        val_loss, val_acc, val_f1 = eval_epoch(model, loaders[\"val\"], criterion)\n",
    "\n",
    "        history[\"train\"].append((tr_acc, tr_f1))\n",
    "        history[\"val\"].append((val_acc, val_f1))\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | Train Acc: {tr_acc:.4f} F1: {tr_f1:.4f} | Val Acc: {val_acc:.4f} F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Test Evaluation\n",
    "    test_loss, test_acc, test_f1 = eval_epoch(model, loaders[\"test\"], criterion)\n",
    "    print(f\"‚úÖ Test Accuracy: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return model, relation2id\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, relation2id = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-14T06:06:22.991585Z",
     "iopub.status.idle": "2025-10-14T06:06:22.991789Z",
     "shell.execute_reply": "2025-10-14T06:06:22.991700Z",
     "shell.execute_reply.started": "2025-10-14T06:06:22.991692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BlipProcessor, BlipModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BLIP backbone (pretrained multimodal)\n",
    "BLIP_NAME = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "# Your MNRE files (adjust if needed)\n",
    "TXT_PATHS = {\n",
    "    'train': '/kaggle/working/MNRE/mnre_txt/mnre_train.txt',\n",
    "    'val':   '/kaggle/working/MNRE/mnre_txt/mnre_val.txt',   # ensure no trailing space in filename\n",
    "    'test':  '/kaggle/working/MNRE/mnre_txt/mnre_test.txt',\n",
    "}\n",
    "IMG_DIRS = {\n",
    "    'train': '/kaggle/working/img_org/train',\n",
    "    'val':   '/kaggle/working/img_org/val',\n",
    "    'test':  '/kaggle/working/img_org/test',\n",
    "}\n",
    "\n",
    "# Your trained RE checkpoint (produced by the BLIP training script)\n",
    "CKPT_PATH = \"mnre_blip_re.pth\"  # change if your filename differs\n",
    "\n",
    "# Example images the user gave\n",
    "IMAGES_TO_PREDICT = [\n",
    "    \"/kaggle/working/img_org/test/twitter_19_31_0_13.jpg\",\n",
    "    \"/kaggle/working/img_org/test/twitter_19_31_0_8.jpg\",\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def safe_fix_val_path():\n",
    "    \"\"\"If the val file was created with an accidental space, rename it once.\"\"\"\n",
    "    bad = \"/kaggle/working/MNRE/mnre_txt/mnre_val .txt\"\n",
    "    good = \"/kaggle/working/MNRE/mnre_txt/mnre_val.txt\"\n",
    "    if os.path.exists(bad) and not os.path.exists(good):\n",
    "        os.rename(bad, good)\n",
    "\n",
    "def load_all_mnre_entries(txt_paths):\n",
    "    \"\"\"Load ALL entries across train/val/test for lookup by img_id.\"\"\"\n",
    "    entries = []\n",
    "    for split, p in txt_paths.items():\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing file for split '{split}': {p}\")\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = ast.literal_eval(line.strip())\n",
    "                obj[\"_split\"] = split\n",
    "                entries.append(obj)\n",
    "    return entries\n",
    "\n",
    "def build_relation2id_from_train(train_path):\n",
    "    \"\"\"Build relation2id by scanning the train file (consistent with training).\"\"\"\n",
    "    rels = set()\n",
    "    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            obj = ast.literal_eval(line.strip())\n",
    "            rels.add(obj[\"relation\"])\n",
    "    relation2id = {r: idx for idx, r in enumerate(sorted(rels))}\n",
    "    return relation2id\n",
    "\n",
    "def insert_markers(tokens, h_pos, t_pos):\n",
    "    \"\"\"Insert [E1] [/E1], [E2] [/E2] into tokens given head/tail spans.\"\"\"\n",
    "    toks = tokens.copy()\n",
    "    spans = [('h', h_pos[0], h_pos[1]), ('t', t_pos[0], t_pos[1])]\n",
    "    spans.sort(key=lambda x: x[1], reverse=True)\n",
    "    for etype, start, end in spans:\n",
    "        tag_close = '[/E1]' if etype == 'h' else '[/E2]'\n",
    "        tag_open  = '[E1]'  if etype == 'h' else '[E2]'\n",
    "        toks.insert(end, tag_close)\n",
    "        toks.insert(start, tag_open)\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def extract_entities_from_marked_text(marked_text):\n",
    "    \"\"\"Return (head_text, tail_text) by parsing [E1]..[/E1] and [E2]..[/E2].\"\"\"\n",
    "    def between(text, open_tag, close_tag):\n",
    "        if open_tag in text and close_tag in text:\n",
    "            s = text.index(open_tag) + len(open_tag)\n",
    "            e = text.index(close_tag)\n",
    "            return text[s:e].strip()\n",
    "        return None\n",
    "    h_text = between(marked_text, \"[E1]\", \"[/E1]\")\n",
    "    t_text = between(marked_text, \"[E2]\", \"[/E2]\")\n",
    "    return h_text, t_text\n",
    "\n",
    "# ----------------------------\n",
    "# BLIP-based RE model definition (same as training)\n",
    "# ----------------------------\n",
    "class BLIPRelationClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification on top of BLIP's projection embeddings.\n",
    "    We use blip.config.projection_dim to stay compatible across variants.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_relations, blip_model):\n",
    "        super().__init__()\n",
    "        self.blip = blip_model\n",
    "        hidden = self.blip.config.projection_dim  # 512 for base captioning model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_relations)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, use_text_embeds=True):\n",
    "        \"\"\"\n",
    "        use_text_embeds=True:\n",
    "            classify using BLIP's text_embeds (projected text representation)\n",
    "        use_text_embeds=False:\n",
    "            classify using BLIP's image_embeds\n",
    "        Optionally, you could concatenate both.\n",
    "        \"\"\"\n",
    "        out = self.blip(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        pixel_values=pixel_values)\n",
    "        pooled = out.text_embeds if use_text_embeds else out.image_embeds\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "# ----------------------------\n",
    "# Load BLIP + Processor + NER pipeline\n",
    "# ----------------------------\n",
    "def load_backbones():\n",
    "    processor = BlipProcessor.from_pretrained(BLIP_NAME)\n",
    "    blip_model = BlipModel.from_pretrained(BLIP_NAME).to(device)\n",
    "    # We typically freeze BLIP for inference; it doesn't matter here.\n",
    "    for p in blip_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # NER model (pretrained token-classification)\n",
    "    ner_name = \"dslim/bert-base-NER\"\n",
    "    ner_tok = AutoTokenizer.from_pretrained(ner_name)\n",
    "    ner_mdl = AutoModelForTokenClassification.from_pretrained(ner_name).to(device)\n",
    "    ner_pipe = pipeline(\"ner\", model=ner_mdl, tokenizer=ner_tok, aggregation_strategy=\"simple\", device=0 if device.type==\"cuda\" else -1)\n",
    "\n",
    "    return processor, blip_model, ner_pipe\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare model from checkpoint (or fall back)\n",
    "# ----------------------------\n",
    "def prepare_re_model(processor, blip_model, relation2id, ckpt_path):\n",
    "    model = BLIPRelationClassifier(num_relations=len(relation2id), blip_model=blip_model).to(device)\n",
    "    if os.path.exists(ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        missing = model.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
    "        if missing.missing_keys or missing.unexpected_keys:\n",
    "            print(\"‚ö†Ô∏è State dict mismatch:\", missing)\n",
    "        id2rel_from_ckpt = {v:k for k,v in ckpt.get(\"relation2id\", {}).items()}\n",
    "        print(f\"‚úÖ Loaded RE checkpoint: {ckpt_path}\")\n",
    "        return model, id2rel_from_ckpt if id2rel_from_ckpt else {v:k for k,v in relation2id.items()}\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è RE checkpoint not found at {ckpt_path}. Using fresh classifier weights (accuracy will be poor).\")\n",
    "        return model, {v:k for k,v in relation2id.items()}\n",
    "\n",
    "# ----------------------------\n",
    "# Build an index: img_id -> list of entries\n",
    "# ----------------------------\n",
    "def index_entries_by_img_id(entries):\n",
    "    idx = {}\n",
    "    for e in entries:\n",
    "        img_id = e[\"img_id\"]\n",
    "        idx.setdefault(img_id, []).append(e)\n",
    "    return idx\n",
    "\n",
    "# ----------------------------\n",
    "# Single-sample preprocessing for BLIP\n",
    "# ----------------------------\n",
    "def build_blip_inputs(processor, image_path, marked_text, max_len=128):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, text=marked_text,\n",
    "                       padding=\"max_length\", truncation=True,\n",
    "                       max_length=max_len, return_tensors=\"pt\")\n",
    "    # Move to device\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(device)\n",
    "    return image, inputs\n",
    "\n",
    "# ----------------------------\n",
    "# Pretty print + visualize\n",
    "# ----------------------------\n",
    "def show_result(image, pred_relation, confidence, sentence, h_text, h_type, t_text, t_type):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Relation: {pred_relation} ({confidence:.1%})\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(\"üìù Sentence:\")\n",
    "    print(sentence)\n",
    "    print(\"\\nüë§ Head Entity (E1):\", h_text, \"| NER type:\", h_type)\n",
    "    print(\"üë• Tail Entity (E2):\", t_text, \"| NER type:\", t_type)\n",
    "    print(\"üîó Predicted Relation:\", pred_relation, f\"(conf={confidence:.4f})\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# NER helper for entity types\n",
    "# ----------------------------\n",
    "def ner_type_for_span(ner_pipe, full_text, span_text):\n",
    "    \"\"\"\n",
    "    Run NER on the full sentence; pick the dominant label covering the given span_text.\n",
    "    If not found, return 'MISC'.\n",
    "    \"\"\"\n",
    "    ents = ner_pipe(full_text)\n",
    "    # Find any entity whose text overlaps with span_text (simple contains)\n",
    "    candidates = [e for e in ents if span_text and span_text in e[\"word\"] or span_text and span_text in e.get(\"entity_group\",\"\")]\n",
    "    if not candidates:\n",
    "        # looser matching: check if entity words are inside the span text\n",
    "        for e in ents:\n",
    "            if e[\"word\"] and e[\"word\"] in span_text:\n",
    "                candidates.append(e)\n",
    "    if not candidates:\n",
    "        return \"MISC\"\n",
    "    # majority label\n",
    "    from collections import Counter\n",
    "    label = Counter([e[\"entity_group\"] for e in candidates]).most_common(1)[0][0]\n",
    "    return label\n",
    "\n",
    "# ----------------------------\n",
    "# Main: predict on a list of image paths\n",
    "# ----------------------------\n",
    "def predict_images(image_paths):\n",
    "    # Fix any bad filename first\n",
    "    safe_fix_val_path()\n",
    "\n",
    "    # Load entries and index by img_id\n",
    "    entries = load_all_mnre_entries(TXT_PATHS)\n",
    "    by_img = index_entries_by_img_id(entries)\n",
    "\n",
    "    # Build relation2id (from train) for consistent id order\n",
    "    relation2id_train = build_relation2id_from_train(TXT_PATHS[\"train\"])\n",
    "\n",
    "    # Backbones\n",
    "    processor, blip_model, ner_pipe = load_backbones()\n",
    "\n",
    "    # RE model\n",
    "    re_model, id2relation = prepare_re_model(processor, blip_model, relation2id_train, CKPT_PATH)\n",
    "    re_model.eval()\n",
    "\n",
    "    # Go over requested images\n",
    "    for ipath in image_paths:\n",
    "        if not os.path.exists(ipath):\n",
    "            print(f\"‚ùå Image not found: {ipath}\")\n",
    "            continue\n",
    "        img_id = os.path.basename(ipath)\n",
    "\n",
    "        if img_id not in by_img:\n",
    "            print(f\"‚ö†Ô∏è No MNRE entry found for image id: {img_id}\")\n",
    "            # Still show the image\n",
    "            image = Image.open(ipath).convert(\"RGB\")\n",
    "            plt.imshow(image); plt.axis('off'); plt.title(\"Image (no matching text entry)\"); plt.show()\n",
    "            continue\n",
    "\n",
    "        # MNRE can have multiple sentences per image; iterate all\n",
    "        for sample in by_img[img_id]:\n",
    "            tokens = sample[\"token\"]\n",
    "            h_pos = sample[\"h\"][\"pos\"]\n",
    "            t_pos = sample[\"t\"][\"pos\"]\n",
    "\n",
    "            # Insert entity markers so model knows E1/E2\n",
    "            marked_text = insert_markers(tokens, h_pos, t_pos)\n",
    "            h_text, t_text = extract_entities_from_marked_text(marked_text)\n",
    "\n",
    "            # Build BLIP inputs\n",
    "            image, inputs = build_blip_inputs(processor, ipath, marked_text, max_len=128)\n",
    "\n",
    "            # RE prediction\n",
    "            with torch.no_grad():\n",
    "                logits = re_model(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"pixel_values\"], use_text_embeds=True)\n",
    "                probs  = torch.softmax(logits, dim=1)\n",
    "                pred_i = int(torch.argmax(probs, dim=1).item())\n",
    "                conf   = float(probs[0, pred_i].item())\n",
    "                pred_relation = id2relation.get(pred_i, f\"rel_{pred_i}\")\n",
    "\n",
    "            # NER types for E1/E2 (using pretrained NER)\n",
    "            # We run NER on the sentence WITHOUT tags for better tagging\n",
    "            clean_sentence = marked_text.replace(\"[E1]\", \"\").replace(\"[/E1]\", \"\").replace(\"[E2]\", \"\").replace(\"[/E2]\", \"\")\n",
    "            h_type = ner_type_for_span(ner_pipe, clean_sentence, h_text or \"\")\n",
    "            t_type = ner_type_for_span(ner_pipe, clean_sentence, t_text or \"\")\n",
    "\n",
    "            # Display\n",
    "            show_result(image, pred_relation, conf, marked_text, h_text, h_type, t_text, t_type)\n",
    "\n",
    "# ----------------------------\n",
    "# RUN\n",
    "# ----------------------------\n",
    "predict_images(IMAGES_TO_PREDICT)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
